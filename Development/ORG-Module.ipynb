{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51ce25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb4bed",
   "metadata": {},
   "source": [
    "# ORG-Module\n",
    "\n",
    "Object Relational Graph is a module that learns to describe an object based on its relationship with others in a video. The algorithm consists many steps and stated in the following order:\n",
    "\n",
    "1. Apply pretrained object detector to capture severall class-agnostic proposal.\n",
    "2. The object features is captured on each keyframes.\n",
    "3. The object features then stored in R, where i is the i-th keyframes, and k is the k-th object.\n",
    "4. The number of objects extracted from each frames are five objects.\n",
    "5. The R variable consist of 5 independent object features.\n",
    "6. Define Object Set R K x d, where K is the number of object nodes, and d is the dimension features.\n",
    "7. Define A, where A is a relation coefficient matrix between K nodes.\n",
    "8. Before feeding to A, the R variable is feed to **Fully connected layer** with bias resulting in R'.\n",
    "9. Then A is the product of fully connected layer between R' and R'T\n",
    "10. After that, the product is activated using softmax function and named A^\n",
    "11. Apply the GCN function, R^ = A^ . R . Wr, Where Wr is learnable parameter\n",
    "12. R^ is the enhanced object features with interaction message between objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4d0bf",
   "metadata": {},
   "source": [
    "# Develop Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6610f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the object feats has the dimension of Frames x Objs x features\n",
    "# with batch dimension it becomes 4-D tensor\n",
    "\n",
    "feat_dims = 512\n",
    "k_object = 5\n",
    "\n",
    "# this means the object is the second object\n",
    "# of the first frame\n",
    "\n",
    "r_obj_feats = torch.rand(k_objects, feat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1bc61afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on ORG paper A is equal to:\n",
    "# φ(R) . transpose(ψ(R))\n",
    "# where : ...\n",
    "# φ(R) = R . Wi + bi\n",
    "# ψ(R) = R . wj + bj\n",
    "\n",
    "in_features = feat_dims\n",
    "out_features = feat_dims\n",
    "\n",
    "sigma_r = nn.Linear(in_features, out_features)\n",
    "psi_r = nn.Linear(in_features, out_features)\n",
    "a_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "w_r = nn.Linear(in_features, out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9ed7e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_r_out = sigma_r(r_obj_feats)\n",
    "psi_r_out = psi_r(r_obj_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "71c75389",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_coeff_mat = torch.matmul(sigma_r_out, torch.t(psi_r_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1333cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat = a_softmax(a_coeff_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "87cf3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat_mul_r = torch.matmul(a_hat, r_obj_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5f6dfb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = w_r(a_hat_mul_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5178e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2951,  0.0876, -0.1391,  ...,  0.0284, -0.2573, -0.5024],\n",
       "        [-0.2546,  0.1451, -0.1221,  ..., -0.0057, -0.2320, -0.4991],\n",
       "        [-0.3415,  0.0536, -0.1762,  ...,  0.0159, -0.2983, -0.4954],\n",
       "        [-0.2284,  0.1758, -0.1091,  ..., -0.0296, -0.2088, -0.4882],\n",
       "        [-0.2611,  0.1293, -0.1218,  ...,  0.0073, -0.2321, -0.4986]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275acd1",
   "metadata": {},
   "source": [
    "# Class Side (Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c1cb33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORG(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_dims):\n",
    "        super(ORG, self).__init__()\n",
    "        '''\n",
    "        Object Relational Graph (ORG) is a module that learns \n",
    "        to describe an object based on its relationship \n",
    "        with others in a video.\n",
    "        \n",
    "        Arguments:\n",
    "            feat_size : The object feature size that obtained from\n",
    "                        the last fully-connected layer of the backbone\n",
    "                        of Faster R-CNN, this case is 512\n",
    "        '''\n",
    "        \n",
    "        sigma_r = nn.Linear(feat_dims, feat_dims)\n",
    "        psi_r = nn.Linear(feat_dims, feat_dims)\n",
    "        \n",
    "        a_softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        w_r = nn.Linear(feat_dims, feat_dims, bias=False)\n",
    "        \n",
    "    def forward(self, r_obj_feat):\n",
    "        sigma_r_out = sigma_r(r_obj_feats)\n",
    "        psi_r_out = psi_r(r_obj_feats)\n",
    "        \n",
    "        a_coeff_mat = torch.matmul(sigma_r_out, torch.t(psi_r_out))\n",
    "        a_hat = a_softmax(a_coeff_mat)\n",
    "        \n",
    "        a_hat_mul_r = torch.matmul(a_hat, r_obj_feats)\n",
    "        output = w_r(a_hat_mul_r)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "10bf2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_module = ORG(feat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bb8ed9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2951,  0.0876, -0.1391,  ...,  0.0284, -0.2573, -0.5024],\n",
       "        [-0.2546,  0.1451, -0.1221,  ..., -0.0057, -0.2320, -0.4991],\n",
       "        [-0.3415,  0.0536, -0.1762,  ...,  0.0159, -0.2983, -0.4954],\n",
       "        [-0.2284,  0.1758, -0.1091,  ..., -0.0296, -0.2088, -0.4882],\n",
       "        [-0.2611,  0.1293, -0.1218,  ...,  0.0073, -0.2321, -0.4986]],\n",
       "       grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_hat = org_module(r_obj_feats)\n",
    "r_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d416e4e",
   "metadata": {},
   "source": [
    "# In Practice Using Faster R-CNN Object Features (Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418c3df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
