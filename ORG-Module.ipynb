{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ce25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb4bed",
   "metadata": {},
   "source": [
    "# ORG-Module\n",
    "\n",
    "Object Relational Graph is a module that learns to describe an object based on its relationship with others in a video. The algorithm consists many steps and stated in the following order:\n",
    "\n",
    "1. Apply pretrained object detector to capture severall class-agnostic proposal.\n",
    "2. The object features is captured on each keyframes.\n",
    "3. The object features then stored in R, where i is the i-th keyframes, and k is the k-th object.\n",
    "4. The number of objects extracted from each frames are five objects.\n",
    "5. The R variable consist of 5 independent object features.\n",
    "6. Define Object Set R K x d, where K is the number of object nodes, and d is the dimension features.\n",
    "7. Define A, where A is a relation coefficient matrix between K nodes.\n",
    "8. Before feeding to A, the R variable is feed to **Fully connected layer** with bias resulting in R'.\n",
    "9. Then A is the product of fully connected layer between R' and R'T\n",
    "10. After that, the product is activated using softmax function and named A^\n",
    "11. Apply the GCN function, R^ = A^ . R . Wr, Where Wr is learnable parameter\n",
    "12. R^ is the enhanced object features with interaction message between objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4d0bf",
   "metadata": {},
   "source": [
    "# Develop Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6610f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the object feats has the dimension of Frames x Objs x features\n",
    "# with batch dimension it becomes 4-D tensor\n",
    "\n",
    "feat_dims = 512\n",
    "k_objects = 5\n",
    "\n",
    "# this means the object is the second object\n",
    "# of the first frame\n",
    "\n",
    "r_obj_feats = torch.rand(k_objects, feat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc61afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on ORG paper A is equal to:\n",
    "# φ(R) . transpose(ψ(R))\n",
    "# where : ...\n",
    "# φ(R) = R . Wi + bi\n",
    "# ψ(R) = R . wj + bj\n",
    "\n",
    "in_features = feat_dims\n",
    "out_features = feat_dims\n",
    "\n",
    "sigma_r = nn.Linear(in_features, out_features)\n",
    "psi_r = nn.Linear(in_features, out_features)\n",
    "a_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "w_r = nn.Linear(in_features, out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed7e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_r_out = sigma_r(r_obj_feats)\n",
    "psi_r_out = psi_r(r_obj_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c75389",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_coeff_mat = torch.matmul(sigma_r_out, torch.t(psi_r_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1333cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat = a_softmax(a_coeff_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87cf3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat_mul_r = torch.matmul(a_hat, r_obj_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6dfb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = w_r(a_hat_mul_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5178e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6568aef2",
   "metadata": {},
   "source": [
    "# BEFORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4aeabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORG_OLD(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(ORG_OLD, self).__init__()\n",
    "        '''\n",
    "        Object Relational Graph (ORG) is a module that learns \n",
    "        to describe an object based on its relationship \n",
    "        with others in a video.\n",
    "        \n",
    "        Arguments:\n",
    "            feat_size : The object feature size that obtained from\n",
    "                        the last fully-connected layer of the backbone\n",
    "                        of Faster R-CNN\n",
    "        '''\n",
    "\n",
    "\n",
    "        self.sigma_r = nn.Linear(cfg.object_projected_size, \n",
    "                                 cfg.object_projected_size)\n",
    "        \n",
    "        self.psi_r = nn.Linear(cfg.object_projected_size, \n",
    "                               cfg.object_projected_size)\n",
    "        \n",
    "        self.a_softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        self.w_r = nn.Linear(cfg.object_projected_size, \n",
    "                             cfg.object_projected_size, \n",
    "                             bias=False)\n",
    "    \n",
    "    def forward(self, r_obj_feats):\n",
    "        r_hat_ith_frame = []\n",
    "        # for loop on every frame\n",
    "        for i in range(r_obj_feats.size(1)):\n",
    "            sigma_r_out = self.sigma_r(r_obj_feats[:, i])\n",
    "            psi_r_out = self.psi_r(r_obj_feats[:, i])\n",
    "\n",
    "            # batch multiplications\n",
    "            a_coeff_mat = torch.bmm(sigma_r_out, psi_r_out.transpose(1, 2))\n",
    "            a_hat = self.a_softmax(a_coeff_mat)\n",
    "            \n",
    "            # batch multiplication\n",
    "            a_hat_mul_r = torch.bmm(a_hat, r_obj_feats[:, i])\n",
    "            output = self.w_r(a_hat_mul_r)\n",
    "            \n",
    "            r_hat_ith_frame.append(output.unsqueeze(1))\n",
    "        \n",
    "        r_hat = torch.cat(r_hat_ith_frame, dim=1)\n",
    "        \n",
    "        return r_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b385ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_old = ORG_OLD(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084889e",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_feats = torch.randn(128 * 7, 512, 28, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cce38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "org_old(r_feats.permute(0, 2, 3, 1))\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "print(f\"Execution time: {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275acd1",
   "metadata": {},
   "source": [
    "# Class Side (Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cb33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigORGTRL:\n",
    "    def __init__(self):\n",
    "        self.object_input_size = 1024\n",
    "        self.object_projected_size = 512\n",
    "        self.object_kernel_size = (1, 1)\n",
    "\n",
    "\n",
    "class ORG(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(ORG, self).__init__()\n",
    "        '''\n",
    "        Object Relational Graph (ORG) is a module that learns \n",
    "        to describe an object based on its relationship \n",
    "        with others in a video.\n",
    "        \n",
    "        Arguments:\n",
    "            feat_size : The object feature size that obtained from\n",
    "                        the last fully-connected layer of the backbone\n",
    "                        of Faster R-CNN\n",
    "        '''\n",
    "        \n",
    "        self.sigma_r = nn.Conv2d(in_channels=cfg.object_projected_size, \n",
    "                                 out_channels=cfg.object_projected_size,\n",
    "                                 kernel_size=cfg.object_kernel_size)\n",
    "        \n",
    "        self.psi_r = nn.Conv2d(in_channels=cfg.object_projected_size, \n",
    "                                 out_channels=cfg.object_projected_size,\n",
    "                                 kernel_size=cfg.object_kernel_size)\n",
    "        \n",
    "        self.w_r = nn.Conv2d(in_channels=cfg.object_projected_size, \n",
    "                             out_channels=cfg.object_projected_size,\n",
    "                             kernel_size=cfg.object_kernel_size,\n",
    "                             bias=False)\n",
    "        \n",
    "    def forward(self, r_feats):\n",
    "        a_coeff = torch.matmul(self.sigma_r(r_feats).permute(0, 2, 3, 1), \n",
    "                               self.psi_r(r_feats).permute(0, 2, 1, 3))\n",
    "        \n",
    "        a_hat = F.softmax(a_coeff, dim=-1)\n",
    "        \n",
    "        r_hat = torch.matmul(a_hat, self.w_r(r_feats).permute(0, 2, 3, 1))\n",
    "        \n",
    "        return r_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02aae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ConfigORGTRL()\n",
    "org = ORG(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a40a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "org(r_feats)\n",
    "\n",
    "end = time.time()\n",
    "total_time = end - start\n",
    "print(f\"Execution time: {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8c17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_r = nn.Conv2d(in_channels=cfg.object_projected_size, \n",
    "                     out_channels=cfg.object_projected_size,\n",
    "                     kernel_size=cfg.object_kernel_size)\n",
    "        \n",
    "psi_r = nn.Conv2d(in_channels=cfg.object_projected_size, \n",
    "                 out_channels=cfg.object_projected_size,\n",
    "                 kernel_size=cfg.object_kernel_size)\n",
    "\n",
    "w_r = nn.Conv2d(in_channels=cfg.object_projected_size, \n",
    "                 out_channels=cfg.object_projected_size,\n",
    "                 kernel_size=cfg.object_kernel_size,\n",
    "                 bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a500c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_r(r_feats).permute(0, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bb1c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_r(r_feats).permute(0, 2, 3, 1).transpose(2, -1)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e1d82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_r(r_feats).permute(0, 2, 1, 3)[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05223410",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat = F.softmax(torch.matmul(sigma_r(r_feats).permute(0, 2, 3, 1), \n",
    "                               psi_r(r_feats).permute(0, 2, 3, 1).transpose(2, -1)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff13d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_hat = torch.bmm(a_hat, w_r(r_feats))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d416e4e",
   "metadata": {},
   "source": [
    "# In Practice Using Faster R-CNN Object Features (Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418c3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a05fc2",
   "metadata": {},
   "source": [
    "# Attention LSTM Class (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b1cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size,\n",
    "                 features_size,\n",
    "                 attn_size):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        '''\n",
    "        Temporal Attention Module of ORG.\n",
    "        It depends on previous hidden state of LSTM attention.\n",
    "        Arguments:\n",
    "          lstm_attn_hidden: The hidden state from LSTM attention\n",
    "                            tensors of shape (batch_size, hidden_size).\n",
    "          video_feats_size: The concatenation of frame features\n",
    "                            and motion features.\n",
    "                            tensors of shape (batch_size, n_frames, feats_size)\n",
    "          attn_size       : The attention size of attention module.\n",
    "        '''\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.features_size = features_size\n",
    "        self.attn_size = attn_size\n",
    "        \n",
    "        # This layer is for the operation between W_a and V_i\n",
    "        # : W_a is the learnable params associated with Video Features\n",
    "        # : V_i is the concatenation between appearance features\n",
    "        #   and motion features\n",
    "        encoder_projection = nn.Linear(features_size, \n",
    "                                       attn_size, \n",
    "                                       bias=False)\n",
    "\n",
    "        # This layer is for the operation between U_a and h_t_attn\n",
    "        # : U_a is the learnable params associated with LSTM attn hidden states\n",
    "        # : h_t_attn is the concatenation between appearance features\n",
    "        #   and motion features\n",
    "        decoder_projection = nn.Linear(hidden_size, \n",
    "                                       attn_size, \n",
    "                                       bias=False)\n",
    "\n",
    "        # This layer is for the operation between w_T and result tanh(W_v + U_h)\n",
    "        # : W_v is the result of matrix multiplication between \n",
    "        #   video features and weight W\n",
    "        # : U_h is the result of matrix multiplication between\n",
    "        #   LSTM attention hidden states and weight U\n",
    "        # : tanh(.) is the tanh activation function\n",
    "        # : w_T is a vector of learnable params for the result\n",
    "        #   of the tanh activation\n",
    "        energy_projection = nn.Linear(attn_size, \n",
    "                                      1, \n",
    "                                      bias=False)\n",
    "        \n",
    "    def forward(self,\n",
    "                h_attn_lstm,\n",
    "                v_features):\n",
    "        '''\n",
    "        shape of hidden attention lstm (batch_size, hidden_size)\n",
    "        shape of video features input (batch_size, n_frames, features_size)\n",
    "        '''\n",
    "        Wv = encoder_projection(v_features)\n",
    "        Uh = decoder_projection(h_attn_lstm)\n",
    "        \n",
    "        Ew = energy_projection(torch.tanh(Wv + Uh))\n",
    "        alpha = softmax_activation(Ew)\n",
    "        \n",
    "        weighted_feats = alpha * v_features\n",
    "        context_global = weighted_feats.sum(dim=1)\n",
    "        \n",
    "        return context_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d866fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1324\n",
    "hidden_size = 512\n",
    "features_size = 512\n",
    "attn_size = 512\n",
    "num_layers= 1 \n",
    "dropout= 0.5\n",
    "\n",
    "lstm_attn = nn.LSTM(input_size, \n",
    "                    hidden_size, \n",
    "                    num_layers, \n",
    "                    batch_first=True, \n",
    "                    dropout=dropout)\n",
    "\n",
    "temporal_attn = TemporalAttention(hidden_size,\n",
    "                                  features_size,\n",
    "                                  attn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199cbae",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_vector = torch.randn(1, 28, 512)\n",
    "motion_vector = torch.randn(1, 28, 512)\n",
    "\n",
    "video_features = torch.cat((feature_vector, motion_vector), dim=1)\n",
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f84b3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_bar = torch.mean(video_features, dim=1, keepdim=True)\n",
    "\n",
    "v_bar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ab41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_word_emb = torch.randn(1, 1, 300)\n",
    "\n",
    "\n",
    "prev_cell_lang_lstm = torch.rand(1, 1, 512)\n",
    "prev_hidden_lang_lstm = torch.rand(1, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393d2240",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_combined = torch.cat((v_bar, prev_word, prev_lang_hidden), dim=-1)\n",
    "prev_h_attn = (prev_hidden_lang_lstm, prev_cell_lang_lstm)\n",
    "\n",
    "input_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf1f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs, hidden_attn_lstm = lstm_attn(input_combined)\n",
    "    context_global = temporal_attn(hidden_attn_lstm[0],\n",
    "                                   video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ac5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_lang = hidden_attn_lstm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f2b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 1\n",
    "\n",
    "last_hidden_lang = last_hidden_lang.view(n_layers, last_hidden_lang.size(1), last_hidden_lang.size(2))\n",
    "# last_hidden_lang = last_hidden_lang[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d9a9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4e62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = 512\n",
    "output_features = 512\n",
    "\n",
    "# This layer is for the operation between W_a and V_i\n",
    "# : W_a is the learnable params associated with Video Features\n",
    "# : V_i is the concatenation between appearance features\n",
    "#   and motion features\n",
    "encoder_projection = nn.Linear(input_features, \n",
    "                               output_features, \n",
    "                               bias=False)\n",
    "\n",
    "# This layer is for the operation between U_a and h_t_attn\n",
    "# : U_a is the learnable params associated with LSTM attn hidden states\n",
    "# : h_t_attn is the concatenation between appearance features\n",
    "#   and motion features\n",
    "decoder_projection = nn.Linear(input_features, \n",
    "                               output_features, \n",
    "                               bias=False)\n",
    "\n",
    "# This layer is for the operation between w_T and result tanh(W_v + U_h)\n",
    "# : W_v is the result of matrix multiplication between \n",
    "#   video features and weight W\n",
    "# : U_h is the result of matrix multiplication between\n",
    "#   LSTM attention hidden states and weight U\n",
    "# : tanh(.) is the tanh activation function\n",
    "# : w_T is a vector of learnable params for the result\n",
    "#   of the tanh activation\n",
    "energy_projection = nn.Linear(input_features, \n",
    "                              1, \n",
    "                              bias=False)\n",
    "\n",
    "# This layer is for the normalization of all the weights\n",
    "# corresponding to its frame\n",
    "softmax_activation = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad0acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wv = encoder_projection(video_features)\n",
    "Uh = decoder_projection(h_attn)\n",
    "Ew = energy_projection(torch.tanh(Wv + Uh))\n",
    "alpha = softmax_activation(Ew)\n",
    "weighted_feats = alpha * video_features\n",
    "context_global = weighted_feats.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af51b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac66495",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 512 \n",
    "hidden_size = 512\n",
    "num_layers= 1 \n",
    "dropout= 0.5\n",
    "\n",
    "attlstm = AttentionLSTM(512,\n",
    "                        512,\n",
    "                        1,\n",
    "                        0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11291ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, hidden = attlstm(prev_word, prev_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da9179",
   "metadata": {},
   "source": [
    "# Temporal Attention (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a209a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 decoder_hidden_size, \n",
    "                 feat_size,\n",
    "                 attn_size,):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        '''\n",
    "        Temporal Attention module. \n",
    "        It depends on previous hidden memory in the decoder(of shape hidden_size),\n",
    "        feature at the source side ( of shape(196, feat_size) ).  \n",
    "        at(s) = align(ht,hs)\n",
    "              = exp(score(ht,hs)) / Sum(exp(score(ht,hs')))  \n",
    "        where\n",
    "        score(ht,hs) = ht.t * hs                         (dot)\n",
    "                     = ht.t * Wa * hs                  (general)\n",
    "                     = va.t * tanh(Wa[ht;hs])           (concat)  \n",
    "        Here we have used concat formulae.\n",
    "        Argumets:\n",
    "          hidden_size : hidden memory size of decoder. (batch,hidden_size)\n",
    "          feat_size : feature size of each grid (annotation vector) at encoder side.\n",
    "          bottleneck_size : intermediate size.\n",
    "        '''\n",
    "\n",
    "\n",
    "        self.hidden_size = decoder_hidden_size\n",
    "        self.feat_size = feat_size\n",
    "        self.bottleneck_size = attn_size\n",
    "        \n",
    "        self.decoder_projection = nn.Linear(self.hidden_size,\n",
    "                                            self.bottleneck_size,\n",
    "                                            bias=False)\n",
    "        self.encoder_projection = nn.Linear(self.feat_size, \n",
    "                                            self.bottleneck_size, \n",
    "                                            bias=False)\n",
    "        self.final_projection = nn.Linear(self.bottleneck_size, \n",
    "                                          1,\n",
    "                                          bias=False)\n",
    "     \n",
    "    def forward(self, hidden, feats):\n",
    "        '''\n",
    "        shape of hidden (hidden_size) (batch,hidden_size) #(100, 512)\n",
    "        shape of feats (batch size, ,feat_size)  #(100, 40, 1536)\n",
    "        '''\n",
    "\n",
    "        Wh = self.decoder_projection(hidden)  \n",
    "        Uv = self.encoder_projection(feats)   \n",
    "        Wh = Wh.unsqueeze(1).expand_as(Uv)\n",
    "\n",
    "        energies = self.final_projection(torch.tanh(Wh + Uv))\n",
    "\n",
    "        weights = F.softmax(energies, dim=1)\n",
    "        weighted_feats = feats * weights.expand_as(feats)\n",
    "        attn_feats = weighted_feats.sum(dim=1)\n",
    "\n",
    "        return attn_feats, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae01d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=512\n",
    "bottleneck_size=512\n",
    "feat_size=512\n",
    "\n",
    "decoder_projection = nn.Linear(hidden_size,\n",
    "                               bottleneck_size,\n",
    "                               bias=False)\n",
    "\n",
    "encoder_projection = nn.Linear(feat_size, \n",
    "                               bottleneck_size, \n",
    "                               bias=False)\n",
    "\n",
    "final_projection = nn.Linear(bottleneck_size, \n",
    "                              1,\n",
    "                              bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a153bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.randn((2, 512))\n",
    "feats = torch.randn((2, 10, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087474ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wh = decoder_projection(hidden)\n",
    "Uv = encoder_projection(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baf6a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Wh.unsqueeze(1).expand_as(Uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = final_projection(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d41a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.nn.softmax(alpha, dim=1)\n",
    "weighted_feats = feats * alpha.expand_as(feats)\n",
    "attn_feats = weighted_feats.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f1de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1d43c",
   "metadata": {},
   "source": [
    "# Object Alignment Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a784d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9666ca55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the object features for all videos have been loaded as a PyTorch tensor,\n",
    "# where the tensor has shape (batch_size, max_num_frames, max_num_objects, object_feature_dim).\n",
    "videos = torch.randn(2, 3, 5, 512)\n",
    "batch_size, max_num_frames, max_num_objects, object_feature_dim = videos.size()\n",
    "# videos = torch.empty((2, 4, 5, 512)).uniform_(0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbaffe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos = videos.to(device) # move tensor to GPU device\n",
    "# %%time\n",
    "start_time = time.time()\n",
    "\n",
    "# Compute the similarity scores between each pair of frames\n",
    "# # similarity_scores = []\n",
    "aligned_objects = []\n",
    "\n",
    "anchor_frame = videos[:, 0].clone().detach()\n",
    "next_frame = videos[:, 1:videos.size(3)].clone().detach()\n",
    "\n",
    "for i in range(1, max_num_frames):\n",
    "    # ini menggunakan R_Enhanced_Features\n",
    "    i_th_frame = videos[:, i].clone().detach()\n",
    "\n",
    "    # Compute the cosine similarity between each object in the anchor frame and the i-th frame.\n",
    "    similarity_scores_i = torch.bmm(anchor_frame, i_th_frame.transpose(1, 2)) / \\\n",
    "                           (torch.norm(anchor_frame, dim=2)[:, :, None] * torch.norm(i_th_frame, dim=2)[:, None, :])\n",
    "    \n",
    "    max_similarities, max_similarity_indices = torch.max(similarity_scores_i, dim=2)\n",
    "    \n",
    "    aligned_objects_i = torch.gather(i_th_frame, \n",
    "                                     dim=1, \n",
    "                                     index=max_similarity_indices[:, :, None].expand(-1, -1, object_feature_dim))\n",
    "    \n",
    "#     similarity_scores.append(similarity_scores_i)\n",
    "    aligned_objects.append(aligned_objects_i.unsqueeze(1))\n",
    "\n",
    "aligned_frames = torch.cat(aligned_objects, dim=1)\n",
    "all_aligned_frames = torch.cat([anchor_frame.unsqueeze(1), aligned_frames], dim=1)\n",
    "\n",
    "# weighted_frames = torch.mul(all_aligned_frames, alpha.unsqueeze(-1))\n",
    "# sum_weighted_frames = torch.sum(weighted_frames, dim=1)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Execution time: {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43b4389",
   "metadata": {},
   "source": [
    "## Optimized Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475a0826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos = videos.to(device) # move tensor to GPU device\n",
    "# %%time\n",
    "start_time = time.time()\n",
    "\n",
    "anchor_frame = videos[:, 0].clone().detach()\n",
    "next_frame = videos[:, 1:videos.size(3)].clone().detach()\n",
    "\n",
    "similarity_score = (torch.matmul(anchor_frame.unsqueeze(1), next_frame.transpose(2, -1)) / \\\n",
    "                    (torch.norm(anchor_frame.unsqueeze(1), dim=-1)[:, :, :, None] * \\\n",
    "                     torch.norm(next_frame, dim=-1)[:, :, None, :]))\n",
    "\n",
    "aligned_frames = torch.gather(next_frame, \n",
    "                              dim=2, \n",
    "                              index=similarity_score.topk(1, -1)[1].\\\n",
    "                              expand(-1, -1, -1, object_feature_dim))\n",
    "\n",
    "all_aligned_frames = torch.cat([anchor_frame.unsqueeze(1), aligned_frames], dim=1)\n",
    "\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Execution time: {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "50e9c9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_object_variable(object_variable, r_hat):\n",
    "    '''\n",
    "    align object modul according to ORG-TRL Paper\n",
    "    refers = https://openaccess.thecvf.com/content_CVPR_2020/papers/Zhang_Object_Relational_Graph_With_Teacher-Recommended_Learning_for_Video_Captioning_CVPR_2020_paper.pdf\n",
    "    args:\n",
    "      object_variable : This is object features exctracted from Faster RCNN\n",
    "    output:\n",
    "      aligned_object_variable\n",
    "    '''\n",
    "    ## Mengambil anchor frame sebagai acuan untuk setiap objek\n",
    "    ## Memisahkan anchor frame dari keseluruhan fitur objek\n",
    "    anchor_frame = object_variable[:, 0].detach()\n",
    "    next_frame = object_variable[:, 1:object_variable.size(1)].detach()\n",
    "    \n",
    "    ## menghitung cosine similarity scores\n",
    "    ## matmul( achor_frame, next_frame ) / | anchor_frame | * | next_frame |\n",
    "    similarity_score = (torch.matmul(anchor_frame.unsqueeze(1), next_frame.transpose(2, -1)) / \\\n",
    "                        (torch.norm(anchor_frame.unsqueeze(1), dim=-1)[:, :, :, None] * \\\n",
    "                         torch.norm(next_frame, dim=-1)[:, :, None, :]))\n",
    "\n",
    "    aligned_frames = torch.gather(r_hat[:, 1:r_hat.size(3)].detach(), \n",
    "                                  dim=2, \n",
    "                                  index=similarity_score.topk(1, -1)[1].\\\n",
    "                                  expand(-1, -1, -1, r_hat.size(-1)))\n",
    "\n",
    "    return torch.cat([r_hat[:, 0].unsqueeze(1), aligned_frames], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03f74c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = torch.randn(128, 28, 5, 1024)\n",
    "r_hat_input = torch.randn(128, 28, 5, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d9f3384",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_hat = align_object_variable(videos.detach(), r_hat_input.detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cfbc3c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_frame = videos[:, 0].detach()\n",
    "next_frame = videos[:, 1:videos.size(1)].detach()\n",
    "\n",
    "## menghitung cosine similarity scores\n",
    "## matmul( achor_frame, next_frame ) / | anchor_frame | * | next_frame |\n",
    "similarity_score = (torch.matmul(anchor_frame.unsqueeze(1), next_frame.transpose(2, -1)) / \\\n",
    "                    (torch.norm(anchor_frame.unsqueeze(1), dim=-1)[:, :, :, None] * \\\n",
    "                     torch.norm(next_frame, dim=-1)[:, :, None, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561300ab",
   "metadata": {},
   "source": [
    "## Method Fix Aligned Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1647fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "anchor_frame = videos[:, 0].clone().detach()\n",
    "i_th_frame = videos[:, 1:videos.size(3)].clone().detach()\n",
    "\n",
    "similarity_scores_ts = F.cosine_similarity(anchor_frame.unsqueeze(2), \n",
    "                                           i_th_frame.view(batch_size, \n",
    "                                                           -1, \n",
    "                                                           object_feature_dim).unsqueeze(1), \n",
    "                                           dim=-1).view(anchor_frame.size(0), \n",
    "                                                        anchor_frame.size(1), \n",
    "                                                        i_th_frame.size(1), \n",
    "                                                        i_th_frame.size(2))\n",
    "\n",
    "aligned_indices_ts = similarity_scores_ts.topk(1, -1)[1].squeeze(-1).transpose(1, -1).unsqueeze(-1)\n",
    "\n",
    "aligned_frames_ts = torch.gather(i_th_frame, \n",
    "                                 dim=2, \n",
    "                                 index=aligned_indices_ts.expand(-1, -1, -1, anchor_frame.size(-1)))\n",
    "\n",
    "all_aligned_frames = torch.cat([anchor_frame.unsqueeze(1), aligned_frames_ts], dim=1)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Execution time: {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620eb450",
   "metadata": {},
   "outputs": [],
   "source": [
    "aligned_indices_ts.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f34c39b",
   "metadata": {},
   "source": [
    "## Method 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5214b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor([[[[0.1, 0.2, 0.3], [0.15, 0.25, 0.35]]],\n",
    "                  \n",
    "                  [[[0.71, 0.72, 0.73], [0.85, 0.88, 0.89]]]])\n",
    "\n",
    "b = torch.tensor([[[[0.15, 0.25, 0.35], \n",
    "                    [0.5, 0.55, 0.58], \n",
    "                    [0.8, 0.85, 0.9],\n",
    "                    [0.87, 0.85, 0.97]],\n",
    "                   \n",
    "                   [[0.15, 0.25, 0.35], \n",
    "                    [0.5, 0.55, 0.58], \n",
    "                    [0.8, 0.85, 0.9],\n",
    "                    [0.87, 0.85, 0.97]],\n",
    "                  \n",
    "                  [[0.15, 0.25, 0.35], \n",
    "                    [0.5, 0.55, 0.58], \n",
    "                    [0.8, 0.85, 0.9],\n",
    "                    [0.87, 0.85, 0.97]]],\n",
    "                  \n",
    "                  [[[0.3, 0.35, 0.3], \n",
    "                    [0.5, 0.55, 0.52], \n",
    "                    [0.95, 0.95, 0.91],\n",
    "                    [0.87, 0.85, 0.97]],\n",
    "                   \n",
    "                   [[0.3, 0.35, 0.3], \n",
    "                    [0.5, 0.55, 0.52], \n",
    "                    [0.95, 0.95, 0.91],\n",
    "                    [0.87, 0.85, 0.97]],\n",
    "                   \n",
    "                   [[0.15, 0.25, 0.35], \n",
    "                    [0.5, 0.55, 0.58], \n",
    "                    [0.8, 0.85, 0.9],\n",
    "                    [0.87, 0.85, 0.97]]],\n",
    "                                   \n",
    "                 ])\n",
    "\n",
    "# F.cosine_similarity(a, b.view(2, 2, 3), dim=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "F.cosine_similarity(a.squeeze(1).unsqueeze(2), b.view(2,-1, 3).unsqueeze(1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d8bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0953c181",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.squeeze(1).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2b1b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.view(2,-1, 3).unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae708a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hasil fungsi ini memiliki arti\n",
    "# batch_size, num_objects, num_frames, num_objects_i_th_frame\n",
    "res.view(2, 2, 3, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b0caa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = torch.empty((2, 4, 5, 512)).uniform_(0, 1)\n",
    "anchor_frame = videos[:, 0].clone().detach()\n",
    "i_th_frame = videos[:, 1:videos.size(3)].clone().detach()\n",
    "\n",
    "similarity_scores = F.cosine_similarity(anchor_frame.unsqueeze(2), i_th_frame.view(2, -1, 512).unsqueeze(1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49aa20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores.view(2, 5, 3, 5)["
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c92cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_frame.unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5702c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_th_frame.view(2, -1, 512).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3871432",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_th_frame.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab10a72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dff41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "a.squeeze(1).unsqueeze(2).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d82146",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e841b287",
   "metadata": {},
   "outputs": [],
   "source": [
    "b.view(2, -1, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977fd572",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_th_frame.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb0215",
   "metadata": {},
   "source": [
    "## Method 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8692aa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "videos = torch.randn(2, 10, 5, 512)\n",
    "batch = 2\n",
    "frames = 10\n",
    "objects = 5\n",
    "objects_feat = 512\n",
    "\n",
    "# Define anchor frame and i-th frame\n",
    "anchor_frame = videos[:, 0]\n",
    "i_th_frame = videos[:, 1:videos.size(3)]\n",
    "\n",
    "# Reshape tensors to compute cosine similarity for each object\n",
    "anchor_frame_reshape = anchor_frame.view(batch, 1, objects, objects_feat)\n",
    "i_th_frame_reshape = i_th_frame.view(batch, frames-1, objects, objects_feat)\n",
    "\n",
    "# Compute cosine similarity for each object\n",
    "cos_similarity = torch.nn.functional.cosine_similarity(anchor_frame_reshape, i_th_frame_reshape, dim=-1)\n",
    "\n",
    "# Print cosine similarity tensor shape\n",
    "print(cos_similarity.shape) # Output: torch.Size([2, 9, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4af82b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "53dcfb21",
   "metadata": {},
   "source": [
    "## Method 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393452c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(anchor_frame, i_th_frame):\n",
    "    return torch.bmm(anchor_frame, i_th_frame.transpose(1, 2)) / \\\n",
    "           (torch.norm(anchor_frame, dim=2)[:, :, None] * torch.norm(i_th_frame, dim=2)[:, None, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3551b76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anchor_frame = videos[:, 0]\n",
    "i_th_frame = videos[:, 1:max_num_frames] \n",
    "\n",
    "similarity_indices_comp = [torch.max(cosine_similarity(anchor_frame, i_th_frame[:, idx]), dim=2)[1] for idx in range(1, i_th_frame.size(1))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbc2552",
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores_comp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650cbaa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i_th_frame[:, 0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "831ea6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.gather(i_th_frame[:, 0], \n",
    "             1, \n",
    "             similarity_scores_comp[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7422e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_size = 512\n",
    "bottleneck_size = 512\n",
    "feat_size = 1836\n",
    "n_frames = 10\n",
    "batch_size = 2\n",
    "\n",
    "hidden_attn = torch.randn(batch_size, decoder_hidden_size)\n",
    "v_feats = torch.randn(batch_size, n_frames, feat_size)\n",
    "\n",
    "temporal_attn = TemporalAttention(decoder_hidden_size, \n",
    "                                  feat_size, \n",
    "                                  bottleneck_size)\n",
    "\n",
    "att_feats, alpha = temporal_attn(hidden_attn, v_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229b36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_projection = nn.Linear(512, \n",
    "                               512, \n",
    "                               bias=False)\n",
    "\n",
    "decoder_projection = nn.Linear(512, \n",
    "                               512, \n",
    "                               bias=False)\n",
    "\n",
    "energy_projection = nn.Linear(512, \n",
    "                              1, \n",
    "                              bias=False)\n",
    "\n",
    "h_attn_lstm = torch.randn(2, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd9f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wv = encoder_projection(sum_weighted_frames)\n",
    "Uh = decoder_projection(h_attn_lstm)\n",
    "Uh = Uh.unsqueeze(1).expand_as(Wv)\n",
    "\n",
    "Ew = energy_projection(torch.tanh(Wv + Uh))\n",
    "beta = F.softmax(Ew, dim=1)\n",
    "\n",
    "weighted_objs = torch.mul(sum_weighted_frames, beta)\n",
    "local_context_feature = torch.sum(weighted_objs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa91c27",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_context_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e818a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b1d4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_projection.weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186cb5a9",
   "metadata": {},
   "source": [
    "# Spatial Attention (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a29065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 decoder_hidden_size, \n",
    "                 feat_size,\n",
    "                 attn_size,):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        '''\n",
    "        Spatial Attention module. \n",
    "        It depends on previous hidden attention memory in the decoder attention,\n",
    "        and the size of object features.  \n",
    "        Argumets:\n",
    "          decoder_hidden_size : hidden memory size of decoder. (batch, hidden_size)\n",
    "          feat_size : feature size of object features.\n",
    "          bottleneck_size : intermediate size.\n",
    "        '''\n",
    "\n",
    "        self.hidden_size = decoder_hidden_size\n",
    "        self.feat_size = feat_size\n",
    "        self.bottleneck_size = attn_size\n",
    "        \n",
    "        self.decoder_projection = nn.Linear(self.hidden_size,\n",
    "                                            self.bottleneck_size,\n",
    "                                            bias=False)\n",
    "        self.encoder_projection = nn.Linear(self.feat_size, \n",
    "                                            self.bottleneck_size, \n",
    "                                            bias=False)\n",
    "        self.energy_projection = nn.Linear(self.bottleneck_size, \n",
    "                                          1,\n",
    "                                          bias=False)\n",
    "     \n",
    "    def forward(self, h_attn_lstm, obj_feats):\n",
    "        '''\n",
    "        shape of hidden (hidden_size) (batch,hidden_size) #(100, 512)\n",
    "        shape of feats (batch size, ,feat_size)  #(100, 40, 1536)\n",
    "        '''\n",
    "\n",
    "        Wv = self.encoder_projection(obj_feats)\n",
    "        Uh = self.decoder_projection(h_attn_lstm)\n",
    "        Uh = Uh.unsqueeze(1).expand_as(Wv)\n",
    "\n",
    "        Ew = self.energy_projection(torch.tanh(Wv + Uh))\n",
    "        alpha = F.softmax(Ew, dim=1)\n",
    "        \n",
    "        weighted_objs = torch.mul(obj_feats, beta)\n",
    "        global_context_feature = torch.sum(weighted_objs, dim=1)\n",
    "\n",
    "        return global_context_feature, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cfb921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_feat_size=512\n",
    "\n",
    "spatial_attention = SpatialAttention(decoder_hidden_size,\n",
    "                                     obj_feat_size,\n",
    "                                     bottleneck_size)\n",
    "\n",
    "global_context_feature, beta = spatial_attention(h_attn_lstm, sum_weighted_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc18c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_context_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569f9127",
   "metadata": {},
   "outputs": [],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c5535",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4d8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from models.ORG_TRL.model import ORG_TRL\n",
    "from models.ORG_TRL.model import Encoder\n",
    "from models.ORG_TRL.model import DecoderRNN\n",
    "from models.ORG_TRL.model import TemporalAttention\n",
    "from models.ORG_TRL.model import SpatialAttention\n",
    "from config import ConfigORGTRL\n",
    "from config import Path\n",
    "from dictionary import Vocabulary\n",
    "from config import Path\n",
    "from data import DataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b6db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    }
   ],
   "source": [
    "cfg = ConfigORGTRL(opt_encoder=True)\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msrvtt'\n",
    "\n",
    "voc = Vocabulary(cfg, gloVe=True)\n",
    "path = Path(cfg, os.getcwd())\n",
    "voc.load()\n",
    "\n",
    "model = ORG_TRL(voc, cfg, path)\n",
    "\n",
    "# data_handler = DataHandler(cfg, path, voc)\n",
    "# train_dset, val_dset, test_dset = data_handler.getDatasets()\n",
    "# train_loader, val_loader, test_loader = data_handler.getDataloader(train_dset, val_dset, test_dset)\n",
    "\n",
    "# for data in train_loader:\n",
    "#     appearance_features, targets, mask, max_length, _, motion_features, _ = data\n",
    "#     print(appearance_features.shape)\n",
    "#     print(targets.shape)\n",
    "#     print(mask.shape)\n",
    "#     print(max_length)\n",
    "#     print(motion_features.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fe55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "appearance_feat = torch.randn(128, 28, 1536)\n",
    "motion_feat = torch.randn(128, 28, 2048)\n",
    "obj_feat = torch.randn(128, 28, 5, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb9e4206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 1.0831 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "v_feats, r_feats, r_hat = model.encoder(appearance_feat, motion_feat, obj_feat)\n",
    "aligned_objects = model.align_object_variable(r_feats, r_hat)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Execution time: {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a68e5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 28, 5, 512])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_hat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1108e138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 28, 5, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "58ebb357",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uh = torch.randn(128, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cdd51263",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = nn.LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680c49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 1\n",
    "batch_size = 32\n",
    "decoder_hidden_size = 512\n",
    "\n",
    "decoder_hidden = torch.zeros(n_layers, \n",
    "                             batch_size,\n",
    "                             decoder_hidden_size)\n",
    "\n",
    "decoder_hidden_attn = (decoder_hidden, decoder_hidden)\n",
    "decoder_hidden_lang = (decoder_hidden, decoder_hidden)\n",
    "\n",
    "decoder_input = torch.LongTensor([[cfg.SOS_token for _ in range(cfg.batch_size)]])\n",
    "appearance_features = torch.randn((32, 28, 512))\n",
    "motion_features = torch.randn((32, 28, 512))\n",
    "\n",
    "v_features = torch.cat((appearance_features, motion_features), dim=-1)\n",
    "\n",
    "v_bar_features = torch.mean(v_features, dim=1, keepdim=True).squeeze(1).unsqueeze(0)\n",
    "\n",
    "embedded = torch.randn((1, 32, 300))\n",
    "\n",
    "input_attn_lstm = torch.cat((v_bar_features, embedded, decoder_hidden_lang[0]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f704e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, h_lang_lstm, h_attn_lstm = decoder(decoder_input,\n",
    "                                               decoder_hidden_attn,\n",
    "                                               decoder_hidden_lang,\n",
    "                                               v_feats, \n",
    "                                               aligned_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54d7407",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model.BeamDecoding(appearance_feat, \n",
    "                            motion_feat, \n",
    "                            obj_feat, \n",
    "                            5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3a79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_size = 512\n",
    "bottleneck_size = 512\n",
    "feat_size = 1836\n",
    "\n",
    "temporal_attention = TemporalAttention(cfg)\n",
    "\n",
    "last_hidden_attn = torch.randn(32, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ecf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_global_vector, alpha = temporal_attention(last_hidden_attn, v_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477a7de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_global_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb263fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, h_lang_lstm, h_attn_lstm = decoder(decoder_input,\n",
    "                                               decoder_hidden_attn,\n",
    "                                               decoder_hidden_lang,\n",
    "                                               v_features\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dfcf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6d0dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_attn_lstm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c942bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [1749, 1649, 1549, 1449]\n",
    "target = [628, 234, 76, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9838781",
   "metadata": {},
   "outputs": [],
   "source": [
    "caption = []\n",
    "gt = []\n",
    "\n",
    "for word in output:\n",
    "    caption.append(voc.index2word.get(word))\n",
    "\n",
    "for word in target:\n",
    "    gt.append(voc.index2word.get(word))\n",
    "    \n",
    "print(caption)\n",
    "print(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_features = torch.randn((32, 28, 512))\n",
    "Uh = torch.rand((1, 32, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f639e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = Uh[0].unsqueeze(1).expand_as(v_features)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d39ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_bar_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22481a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_lstm = nn.LSTM(input_size=1836, \n",
    "                         hidden_size=512,\n",
    "                         num_layers=1, \n",
    "                         dropout=0.2,\n",
    "                         batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f7e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, h = attention_lstm(input_attn_lstm,\n",
    "                           decoder_hidden_attn)\n",
    "\n",
    "h[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,\n",
    "            inputs, \n",
    "            attn_hidden,\n",
    "            lang_hidden, \n",
    "            v_features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.cat((v_bar_features, embedded), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8498454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_targets = targets.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2106fa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac10337",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, (key, value) in enumerate(voc.index2word.items()):\n",
    "    if e < 11:\n",
    "        print(e, key, value)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7d2d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7111c74a",
   "metadata": {},
   "source": [
    "# Encoder Object Projection\n",
    "\n",
    "time execution: 2.13 s for ~1000 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c84b6208",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "seq_len = 5\n",
    "in_channels = 1024\n",
    "out_channels = 512\n",
    "num_frames = 28\n",
    "num_boxes = 5\n",
    "num_feats = 512\n",
    "kernel_size = 1\n",
    "feature_dim = 1024\n",
    "\n",
    "obj_feats = torch.randn(batch_size, num_frames, num_boxes, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3db16c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfigORGTRL:\n",
    "    def __init__(self):\n",
    "        self.object_input_size = 1024\n",
    "        self.object_projected_size = 512\n",
    "        self.object_kernel_size = (1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f7db18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super(Encoder,self).__init__()\n",
    "        '''\n",
    "        Encoder module. Project the video feature into a different space which will be \n",
    "        send to decoder.\n",
    "        Argumets:\n",
    "          input_size : Faster RCNN extracted features that has shape 1024-D\n",
    "          output_size : Dimention of projected space.\n",
    "        '''\n",
    "        self.object_projection = nn.Conv2d(cfg.object_input_size, \n",
    "                                           cfg.object_projected_size, \n",
    "                                           cfg.object_kernel_size)\n",
    "           \n",
    "    def forward(self, appearance_feat, motion_feat, object_feat):        \n",
    "        ## Intinya memproyeksikan dengan input\n",
    "        ## yang direshape langsung menjadi (batch_size * num_objects, dim_feature, frame_len)\n",
    "        object_feat = F.relu(self.object_projection(object_feat.permute(0, 3, 1, 2)))\n",
    "                \n",
    "        return appearance_feat, motion_feat, object_feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886ba1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Memeriksa bentuk tensor\n",
    "batch_size, frame_len, num_objects, dim_feature = obj_feats[:2, :3, :2, :5].size()\n",
    "obj_feats[:2, :3, :2, :5].reshape(batch_size * num_objects, dim_feature, frame_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8990747",
   "metadata": {},
   "outputs": [],
   "source": [
    "## inisialisasi nn.Conv1d\n",
    "object_projection = nn.Conv2d(in_channels=in_channels, \n",
    "                              out_channels=out_channels,\n",
    "                              kernel_size=(1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac1e124",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, frame_len, num_objects, dim_feature = obj_feats.size()\n",
    "result = F.relu(object_projection(obj_feats.view(batch_size, dim_feature, frame_len, num_objects)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7210e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = ConfigORGTRL()\n",
    "encoder = Encoder(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b67dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, r_feats = encoder(1, 2, obj_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ab65c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "r_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b5fe07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bdf110",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "encoder(1, 2, obj_feats)\n",
    "\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(f\"Execution time: {total_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8dd9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_feats[0][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548845fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_feats.permute(0, 3, 1, 2)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
