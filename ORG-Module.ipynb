{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ce25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb4bed",
   "metadata": {},
   "source": [
    "# ORG-Module\n",
    "\n",
    "Object Relational Graph is a module that learns to describe an object based on its relationship with others in a video. The algorithm consists many steps and stated in the following order:\n",
    "\n",
    "1. Apply pretrained object detector to capture severall class-agnostic proposal.\n",
    "2. The object features is captured on each keyframes.\n",
    "3. The object features then stored in R, where i is the i-th keyframes, and k is the k-th object.\n",
    "4. The number of objects extracted from each frames are five objects.\n",
    "5. The R variable consist of 5 independent object features.\n",
    "6. Define Object Set R K x d, where K is the number of object nodes, and d is the dimension features.\n",
    "7. Define A, where A is a relation coefficient matrix between K nodes.\n",
    "8. Before feeding to A, the R variable is feed to **Fully connected layer** with bias resulting in R'.\n",
    "9. Then A is the product of fully connected layer between R' and R'T\n",
    "10. After that, the product is activated using softmax function and named A^\n",
    "11. Apply the GCN function, R^ = A^ . R . Wr, Where Wr is learnable parameter\n",
    "12. R^ is the enhanced object features with interaction message between objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4d0bf",
   "metadata": {},
   "source": [
    "# Develop Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6610f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the object feats has the dimension of Frames x Objs x features\n",
    "# with batch dimension it becomes 4-D tensor\n",
    "\n",
    "feat_dims = 512\n",
    "k_objects = 5\n",
    "\n",
    "# this means the object is the second object\n",
    "# of the first frame\n",
    "\n",
    "r_obj_feats = torch.rand(k_objects, feat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc61afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on ORG paper A is equal to:\n",
    "# φ(R) . transpose(ψ(R))\n",
    "# where : ...\n",
    "# φ(R) = R . Wi + bi\n",
    "# ψ(R) = R . wj + bj\n",
    "\n",
    "in_features = feat_dims\n",
    "out_features = feat_dims\n",
    "\n",
    "sigma_r = nn.Linear(in_features, out_features)\n",
    "psi_r = nn.Linear(in_features, out_features)\n",
    "a_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "w_r = nn.Linear(in_features, out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed7e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_r_out = sigma_r(r_obj_feats)\n",
    "psi_r_out = psi_r(r_obj_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c75389",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_coeff_mat = torch.matmul(sigma_r_out, torch.t(psi_r_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1333cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat = a_softmax(a_coeff_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87cf3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat_mul_r = torch.matmul(a_hat, r_obj_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6dfb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = w_r(a_hat_mul_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5178e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.9213e-02, -7.8978e-02,  7.4341e-02,  ...,  3.1808e-01,\n",
       "          1.4916e-01,  2.8370e-01],\n",
       "        [-1.1797e-01, -1.4182e-01,  4.4578e-02,  ...,  3.4157e-01,\n",
       "          8.7550e-02,  2.5541e-01],\n",
       "        [-1.4583e-01, -8.9589e-02,  2.8276e-04,  ...,  3.0307e-01,\n",
       "          1.1692e-02,  1.8906e-01],\n",
       "        [-1.2058e-01, -6.3393e-02,  8.3677e-02,  ...,  3.1532e-01,\n",
       "          1.4132e-01,  2.3636e-01],\n",
       "        [-6.8676e-02, -1.1910e-01,  7.7551e-02,  ...,  3.4590e-01,\n",
       "          1.5732e-01,  3.2165e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275acd1",
   "metadata": {},
   "source": [
    "# Class Side (Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1cb33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORG(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_dims):\n",
    "        super(ORG, self).__init__()\n",
    "        '''\n",
    "        Object Relational Graph (ORG) is a module that learns \n",
    "        to describe an object based on its relationship \n",
    "        with others in a video.\n",
    "        \n",
    "        Arguments:\n",
    "            feat_size : The object feature size that obtained from\n",
    "                        the last fully-connected layer of the backbone\n",
    "                        of Faster R-CNN\n",
    "        '''\n",
    "        \n",
    "        sigma_r = nn.Linear(feat_dims, feat_dims)\n",
    "        psi_r = nn.Linear(feat_dims, feat_dims)\n",
    "        \n",
    "        a_softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        w_r = nn.Linear(feat_dims, feat_dims, bias=False)\n",
    "        \n",
    "    def forward(self, r_obj_feat):\n",
    "        sigma_r_out = sigma_r(r_obj_feats)\n",
    "        psi_r_out = psi_r(r_obj_feats)\n",
    "        \n",
    "        a_coeff_mat = torch.matmul(sigma_r_out, torch.t(psi_r_out))\n",
    "        a_hat = a_softmax(a_coeff_mat)\n",
    "        \n",
    "        a_hat_mul_r = torch.matmul(a_hat, r_obj_feats)\n",
    "        output = w_r(a_hat_mul_r)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bf2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_module = ORG(feat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb8ed9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.9213e-02, -7.8978e-02,  7.4341e-02,  ...,  3.1808e-01,\n",
       "          1.4916e-01,  2.8370e-01],\n",
       "        [-1.1797e-01, -1.4182e-01,  4.4578e-02,  ...,  3.4157e-01,\n",
       "          8.7550e-02,  2.5541e-01],\n",
       "        [-1.4583e-01, -8.9589e-02,  2.8276e-04,  ...,  3.0307e-01,\n",
       "          1.1692e-02,  1.8906e-01],\n",
       "        [-1.2058e-01, -6.3393e-02,  8.3677e-02,  ...,  3.1532e-01,\n",
       "          1.4132e-01,  2.3636e-01],\n",
       "        [-6.8676e-02, -1.1910e-01,  7.7551e-02,  ...,  3.4590e-01,\n",
       "          1.5732e-01,  3.2165e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_hat = org_module(r_obj_feats)\n",
    "r_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bdd6ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d416e4e",
   "metadata": {},
   "source": [
    "# In Practice Using Faster R-CNN Object Features (Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418c3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a05fc2",
   "metadata": {},
   "source": [
    "# Attention LSTM Class (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "26b1cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size,\n",
    "                 features_size,\n",
    "                 attn_size):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        '''\n",
    "        Temporal Attention Module of ORG.\n",
    "        It depends on previous hidden state of LSTM attention.\n",
    "        Arguments:\n",
    "          lstm_attn_hidden: The hidden state from LSTM attention\n",
    "                            tensors of shape (batch_size, hidden_size).\n",
    "          video_feats_size: The concatenation of frame features\n",
    "                            and motion features.\n",
    "                            tensors of shape (batch_size, n_frames, feats_size)\n",
    "          attn_size       : The attention size of attention module.\n",
    "        '''\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.features_size = features_size\n",
    "        self.attn_size = attn_size\n",
    "        \n",
    "        # This layer is for the operation between W_a and V_i\n",
    "        # : W_a is the learnable params associated with Video Features\n",
    "        # : V_i is the concatenation between appearance features\n",
    "        #   and motion features\n",
    "        encoder_projection = nn.Linear(features_size, \n",
    "                                       attn_size, \n",
    "                                       bias=False)\n",
    "\n",
    "        # This layer is for the operation between U_a and h_t_attn\n",
    "        # : U_a is the learnable params associated with LSTM attn hidden states\n",
    "        # : h_t_attn is the concatenation between appearance features\n",
    "        #   and motion features\n",
    "        decoder_projection = nn.Linear(hidden_size, \n",
    "                                       attn_size, \n",
    "                                       bias=False)\n",
    "\n",
    "        # This layer is for the operation between w_T and result tanh(W_v + U_h)\n",
    "        # : W_v is the result of matrix multiplication between \n",
    "        #   video features and weight W\n",
    "        # : U_h is the result of matrix multiplication between\n",
    "        #   LSTM attention hidden states and weight U\n",
    "        # : tanh(.) is the tanh activation function\n",
    "        # : w_T is a vector of learnable params for the result\n",
    "        #   of the tanh activation\n",
    "        energy_projection = nn.Linear(attn_size, \n",
    "                                      1, \n",
    "                                      bias=False)\n",
    "        \n",
    "    def forward(self,\n",
    "                h_attn_lstm,\n",
    "                v_features):\n",
    "        '''\n",
    "        shape of hidden attention lstm (batch_size, hidden_size)\n",
    "        shape of video features input (batch_size, n_frames, features_size)\n",
    "        '''\n",
    "        Wv = encoder_projection(v_features)\n",
    "        Uh = decoder_projection(h_attn_lstm)\n",
    "        \n",
    "        Ew = energy_projection(torch.tanh(Wv + Uh))\n",
    "        alpha = softmax_activation(Ew)\n",
    "        \n",
    "        weighted_feats = alpha * v_features\n",
    "        context_global = weighted_feats.sum(dim=1)\n",
    "        \n",
    "        return context_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "85d866fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1324\n",
    "hidden_size = 512\n",
    "features_size = 512\n",
    "attn_size = 512\n",
    "num_layers= 1 \n",
    "dropout= 0.5\n",
    "\n",
    "lstm_attn = nn.LSTM(input_size, \n",
    "                    hidden_size, \n",
    "                    num_layers, \n",
    "                    batch_first=True, \n",
    "                    dropout=dropout)\n",
    "\n",
    "temporal_attn = TemporalAttention(hidden_size,\n",
    "                                  features_size,\n",
    "                                  attn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1199cbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56, 512])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector = torch.randn(1, 28, 512)\n",
    "motion_vector = torch.randn(1, 28, 512)\n",
    "\n",
    "video_features = torch.cat((feature_vector, motion_vector), dim=1)\n",
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6f84b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_bar = torch.mean(video_features, dim=1, keepdim=True)\n",
    "\n",
    "v_bar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d6ab41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_word_emb = torch.randn(1, 1, 300)\n",
    "\n",
    "\n",
    "prev_cell_lang_lstm = torch.rand(1, 1, 512)\n",
    "prev_hidden_lang_lstm = torch.rand(1, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "393d2240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1324])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_combined = torch.cat((v_bar, prev_word, prev_lang_hidden), dim=-1)\n",
    "prev_h_attn = (prev_hidden_lang_lstm, prev_cell_lang_lstm)\n",
    "\n",
    "input_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1cf1f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs, hidden_attn_lstm = lstm_attn(input_combined)\n",
    "    context_global = temporal_attn(hidden_attn_lstm[0],\n",
    "                                   video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "61ac5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_lang = hidden_attn_lstm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f3f2b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 1\n",
    "\n",
    "last_hidden_lang = last_hidden_lang.view(n_layers, last_hidden_lang.size(1), last_hidden_lang.size(2))\n",
    "# last_hidden_lang = last_hidden_lang[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "17d9a9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f4e62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = 512\n",
    "output_features = 512\n",
    "\n",
    "# This layer is for the operation between W_a and V_i\n",
    "# : W_a is the learnable params associated with Video Features\n",
    "# : V_i is the concatenation between appearance features\n",
    "#   and motion features\n",
    "encoder_projection = nn.Linear(input_features, \n",
    "                               output_features, \n",
    "                               bias=False)\n",
    "\n",
    "# This layer is for the operation between U_a and h_t_attn\n",
    "# : U_a is the learnable params associated with LSTM attn hidden states\n",
    "# : h_t_attn is the concatenation between appearance features\n",
    "#   and motion features\n",
    "decoder_projection = nn.Linear(input_features, \n",
    "                               output_features, \n",
    "                               bias=False)\n",
    "\n",
    "# This layer is for the operation between w_T and result tanh(W_v + U_h)\n",
    "# : W_v is the result of matrix multiplication between \n",
    "#   video features and weight W\n",
    "# : U_h is the result of matrix multiplication between\n",
    "#   LSTM attention hidden states and weight U\n",
    "# : tanh(.) is the tanh activation function\n",
    "# : w_T is a vector of learnable params for the result\n",
    "#   of the tanh activation\n",
    "energy_projection = nn.Linear(input_features, \n",
    "                              1, \n",
    "                              bias=False)\n",
    "\n",
    "# This layer is for the normalization of all the weights\n",
    "# corresponding to its frame\n",
    "softmax_activation = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0ad0acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wv = encoder_projection(video_features)\n",
    "Uh = decoder_projection(h_attn)\n",
    "Ew = energy_projection(torch.tanh(Wv + Uh))\n",
    "alpha = softmax_activation(Ew)\n",
    "weighted_feats = alpha * video_features\n",
    "context_global = weighted_feats.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9af51b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac66495",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 512 \n",
    "hidden_size = 512\n",
    "num_layers= 1 \n",
    "dropout= 0.5\n",
    "\n",
    "attlstm = AttentionLSTM(512,\n",
    "                        512,\n",
    "                        1,\n",
    "                        0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11291ec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'memory' and 'video_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18392/815298799.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_combined\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'memory' and 'video_features'"
     ]
    }
   ],
   "source": [
    "output, hidden = attlstm(prev_word, prev_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da9179",
   "metadata": {},
   "source": [
    "# Temporal Attention (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a209a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 decoder_hidden_size, \n",
    "                 feat_size,\n",
    "                 attn_size,):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        '''\n",
    "        Temporal Attention module. \n",
    "        It depends on previous hidden memory in the decoder(of shape hidden_size),\n",
    "        feature at the source side ( of shape(196, feat_size) ).  \n",
    "        at(s) = align(ht,hs)\n",
    "              = exp(score(ht,hs)) / Sum(exp(score(ht,hs')))  \n",
    "        where\n",
    "        score(ht,hs) = ht.t * hs                         (dot)\n",
    "                     = ht.t * Wa * hs                  (general)\n",
    "                     = va.t * tanh(Wa[ht;hs])           (concat)  \n",
    "        Here we have used concat formulae.\n",
    "        Argumets:\n",
    "          hidden_size : hidden memory size of decoder. (batch,hidden_size)\n",
    "          feat_size : feature size of each grid (annotation vector) at encoder side.\n",
    "          bottleneck_size : intermediate size.\n",
    "        '''\n",
    "\n",
    "\n",
    "        self.hidden_size = decoder_hidden_size\n",
    "        self.feat_size = feat_size\n",
    "        self.bottleneck_size = attn_size\n",
    "        \n",
    "        self.decoder_projection = nn.Linear(self.hidden_size,\n",
    "                                            self.bottleneck_size,\n",
    "                                            bias=False)\n",
    "        self.encoder_projection = nn.Linear(self.feat_size, \n",
    "                                            self.bottleneck_size, \n",
    "                                            bias=False)\n",
    "        self.final_projection = nn.Linear(self.bottleneck_size, \n",
    "                                          1,\n",
    "                                          bias=False)\n",
    "     \n",
    "    def forward(self, hidden, feats):\n",
    "        '''\n",
    "        shape of hidden (hidden_size) (batch,hidden_size) #(100, 512)\n",
    "        shape of feats (batch size, ,feat_size)  #(100, 40, 1536)\n",
    "        '''\n",
    "\n",
    "        Wh = self.decoder_projection(hidden)  \n",
    "        Uv = self.encoder_projection(feats)   \n",
    "        Wh = Wh.unsqueeze(1).expand_as(Uv)\n",
    "\n",
    "        energies = self.final_projection(torch.tanh(Wh + Uv))\n",
    "\n",
    "        weights = F.softmax(energies, dim=1)\n",
    "        weighted_feats = feats * weights.expand_as(feats)\n",
    "        attn_feats = weighted_feats.sum(dim=1)\n",
    "\n",
    "        return attn_feats, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "7ae01d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=512\n",
    "bottleneck_size=512\n",
    "feat_size=512\n",
    "\n",
    "decoder_projection = nn.Linear(hidden_size,\n",
    "                               bottleneck_size,\n",
    "                               bias=False)\n",
    "\n",
    "encoder_projection = nn.Linear(feat_size, \n",
    "                               bottleneck_size, \n",
    "                               bias=False)\n",
    "\n",
    "final_projection = nn.Linear(bottleneck_size, \n",
    "                              1,\n",
    "                              bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "84a153bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.randn((2, 512))\n",
    "feats = torch.randn((2, 10, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "087474ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wh = decoder_projection(hidden)\n",
    "Uv = encoder_projection(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "baf6a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Wh.unsqueeze(1).expand_as(Uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "bd3e1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = final_projection(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "60d41a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.nn.softmax(alpha, dim=1)\n",
    "weighted_feats = feats * alpha.expand_as(feats)\n",
    "attn_feats = weighted_feats.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "583f1de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 255,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attn_feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d1d43c",
   "metadata": {},
   "source": [
    "# Object Alignment Unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "dbaffe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume that the object features for all videos have been loaded as a PyTorch tensor,\n",
    "# where the tensor has shape (batch_size, max_num_frames, max_num_objects, object_feature_dim).\n",
    "videos = torch.randn(2, 10, 5, 512)\n",
    "# videos = videos.to(device) # move tensor to GPU device\n",
    "\n",
    "batch_size, max_num_frames, max_num_objects, object_feature_dim = videos.size()\n",
    "\n",
    "# Compute the similarity scores between each pair of frames\n",
    "similarity_scores = []\n",
    "aligned_objects = []\n",
    "\n",
    "for i in range(1, max_num_frames):\n",
    "    anchor_frame = videos[:, 0]\n",
    "    i_th_frame = videos[:, i] # ini menggunakan R_Enhanced_Features\n",
    "\n",
    "    # Compute the cosine similarity between each object in the anchor frame and the i-th frame.\n",
    "    # Using R_features\n",
    "    similarity_scores_i = torch.bmm(anchor_frame, i_th_frame.transpose(1, 2)) / \\\n",
    "                           (torch.norm(anchor_frame, dim=2)[:, :, None] * torch.norm(i_th_frame, dim=2)[:, None, :])\n",
    "    \n",
    "    max_similarities, max_similarity_indices = torch.max(similarity_scores_i, dim=2)\n",
    "    \n",
    "    aligned_objects_i = torch.gather(i_th_frame, \n",
    "                                     dim=1, \n",
    "                                     index=max_similarity_indices[:, :, None].expand(-1, -1, object_feature_dim))\n",
    "    \n",
    "    similarity_scores.append(similarity_scores_i)\n",
    "    aligned_objects.append(aligned_objects_i.unsqueeze(1))\n",
    "\n",
    "aligned_frames = torch.cat(aligned_objects, dim=1)\n",
    "all_aligned_frames = torch.cat([anchor_frame.unsqueeze(1), aligned_frames], dim=1)\n",
    "\n",
    "weighted_frames = torch.mul(all_aligned_frames, alpha.unsqueeze(-1))\n",
    "sum_weighted_frames = torch.sum(weighted_frames, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "7422e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_size = 512\n",
    "bottleneck_size = 512\n",
    "feat_size = 1836\n",
    "n_frames = 10\n",
    "batch_size = 2\n",
    "\n",
    "hidden_attn = torch.randn(batch_size, decoder_hidden_size)\n",
    "v_feats = torch.randn(batch_size, n_frames, feat_size)\n",
    "\n",
    "temporal_attn = TemporalAttention(decoder_hidden_size, \n",
    "                                  feat_size, \n",
    "                                  bottleneck_size)\n",
    "\n",
    "att_feats, alpha = temporal_attn(hidden_attn, v_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "229b36dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_projection = nn.Linear(512, \n",
    "                               512, \n",
    "                               bias=False)\n",
    "\n",
    "decoder_projection = nn.Linear(512, \n",
    "                               512, \n",
    "                               bias=False)\n",
    "\n",
    "energy_projection = nn.Linear(512, \n",
    "                              1, \n",
    "                              bias=False)\n",
    "\n",
    "h_attn_lstm = torch.randn(2, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "9fd9f2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wv = encoder_projection(sum_weighted_frames)\n",
    "Uh = decoder_projection(h_attn_lstm)\n",
    "Uh = Uh.unsqueeze(1).expand_as(Wv)\n",
    "\n",
    "Ew = energy_projection(torch.tanh(Wv + Uh))\n",
    "beta = F.softmax(Ew, dim=1)\n",
    "\n",
    "weighted_objs = torch.mul(sum_weighted_frames, beta)\n",
    "local_context_feature = torch.sum(weighted_objs, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "0aa91c27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_context_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "48e818a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 512])"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "0b1d4eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([512, 512])"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_projection.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ed4205b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "186cb5a9",
   "metadata": {},
   "source": [
    "# Spatial Attention (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "d6a29065",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 decoder_hidden_size, \n",
    "                 feat_size,\n",
    "                 attn_size,):\n",
    "        super(SpatialAttention, self).__init__()\n",
    "        '''\n",
    "        Spatial Attention module. \n",
    "        It depends on previous hidden attention memory in the decoder attention,\n",
    "        and the size of object features.  \n",
    "        Argumets:\n",
    "          decoder_hidden_size : hidden memory size of decoder. (batch, hidden_size)\n",
    "          feat_size : feature size of object features.\n",
    "          bottleneck_size : intermediate size.\n",
    "        '''\n",
    "\n",
    "        self.hidden_size = decoder_hidden_size\n",
    "        self.feat_size = feat_size\n",
    "        self.bottleneck_size = attn_size\n",
    "        \n",
    "        self.decoder_projection = nn.Linear(self.hidden_size,\n",
    "                                            self.bottleneck_size,\n",
    "                                            bias=False)\n",
    "        self.encoder_projection = nn.Linear(self.feat_size, \n",
    "                                            self.bottleneck_size, \n",
    "                                            bias=False)\n",
    "        self.energy_projection = nn.Linear(self.bottleneck_size, \n",
    "                                          1,\n",
    "                                          bias=False)\n",
    "     \n",
    "    def forward(self, h_attn_lstm, obj_feats):\n",
    "        '''\n",
    "        shape of hidden (hidden_size) (batch,hidden_size) #(100, 512)\n",
    "        shape of feats (batch size, ,feat_size)  #(100, 40, 1536)\n",
    "        '''\n",
    "\n",
    "        Wv = self.encoder_projection(obj_feats)\n",
    "        Uh = self.decoder_projection(h_attn_lstm)\n",
    "        Uh = Uh.unsqueeze(1).expand_as(Wv)\n",
    "\n",
    "        Ew = self.energy_projection(torch.tanh(Wv + Uh))\n",
    "        alpha = F.softmax(Ew, dim=1)\n",
    "        \n",
    "        weighted_objs = torch.mul(obj_feats, beta)\n",
    "        global_context_feature = torch.sum(weighted_objs, dim=1)\n",
    "\n",
    "        return global_context_feature, beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "id": "8cfb921f",
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_feat_size=512\n",
    "\n",
    "spatial_attention = SpatialAttention(decoder_hidden_size,\n",
    "                                     obj_feat_size,\n",
    "                                     bottleneck_size)\n",
    "\n",
    "global_context_feature, beta = spatial_attention(h_attn_lstm, sum_weighted_frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "3fc18c16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 512])"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_context_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "569f9127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 5, 1])"
      ]
     },
     "execution_count": 328,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df9c5535",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ef4d8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from models.ORG_TRL.model import Encoder\n",
    "from models.ORG_TRL.model import DecoderRNN\n",
    "from models.ORG_TRL.model import TemporalAttention\n",
    "from config import ConfigORGTRL\n",
    "from config import Path\n",
    "from dictionary import Vocabulary\n",
    "from config import Path\n",
    "from data import DataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d9b6db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 1536])\n",
      "torch.Size([14, 32])\n",
      "torch.Size([14, 32])\n",
      "14\n",
      "torch.Size([32, 28, 2048])\n"
     ]
    }
   ],
   "source": [
    "cfg = ConfigORGTRL(opt_encoder=True)\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msrvtt'\n",
    "\n",
    "voc = Vocabulary(cfg, gloVe=True)\n",
    "path = Path(cfg, os.getcwd())\n",
    "voc.load()\n",
    "\n",
    "encoder = Encoder(cfg)\n",
    "decoder = DecoderRNN(cfg, voc)\n",
    "\n",
    "data_handler = DataHandler(cfg, path, voc)\n",
    "train_dset, val_dset, test_dset = data_handler.getDatasets()\n",
    "train_loader, val_loader, test_loader = data_handler.getDataloader(train_dset, val_dset, test_dset)\n",
    "\n",
    "for data in train_loader:\n",
    "    appearance_features, targets, mask, max_length, _, motion_features, _ = data\n",
    "    print(appearance_features.shape)\n",
    "    print(targets.shape)\n",
    "    print(mask.shape)\n",
    "    print(max_length)\n",
    "    print(motion_features.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb2bf2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = targets[0].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "95587d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   6,  255,   77,    6,  140,    6,    6,    6,    6,    6,    6,    6,\n",
       "            6,   77,   92,    6,    6,    6,    6,  135,    6,    6,  451,   11,\n",
       "            6,    9, 2708,    6,    6,  353,    6, 3782],\n",
       "        [  14,   25,   25,   92,   25,   92,  263,   85,  202,   92,  525,   79,\n",
       "         1649,   25,  462,  469,   61, 1340,   92,  187,  165,  263,    6,   21,\n",
       "         2303,   28, 2707,  219,   87,  167,  630, 3780],\n",
       "        [  15,   34,  255,   25,    6,   25,  629,   86,  249,  283,  330,  283,\n",
       "           25,    6,   70,   92,   25,   79,  283,  195,  299, 1117,  187,   22,\n",
       "           25,  996,    5,   34,  103,  748,    6,   25],\n",
       "        [  53,   99, 3622,   97,   99, 1428,    6,   87, 2016,   12,  235,  748,\n",
       "          607,   92,  243,   34,  624,   25,  198,   17,  247, 1115,   99,  499,\n",
       "          352,   28,   28,  117, 1850,   28, 2462,  579],\n",
       "        [  21,  206,   39,    2,   12,   39,  519,    2,    6, 2383,   28,    6,\n",
       "            2,  631,    6,    6,    2,  117,   28,    2,   66, 1118,    2,   21,\n",
       "           70, 1244, 2110,   70, 3132,  932,  537,   39],\n",
       "        [ 345,    2,    6,    0,   28,    6,    2,    0, 1535, 2366, 2078,   99,\n",
       "            0,    6,   99,  206,    0,   12,  118,    0,    6,   70,    0,  332,\n",
       "            6,   25,    2,   28,    2,    2,  446,   28],\n",
       "        [ 671,    0, 1510,    0, 3811,   99,    0,    0,   21, 2379,    2,  206,\n",
       "            0,  478,  206,   15,    0,  139,    2,    0,  202,  284,    0,   66,\n",
       "           92, 2306,    0, 1154,    0,    0,   39,  589],\n",
       "        [  21,    0,  429,    0, 3723,  336,    0,    0,    6,    5,    0,    2,\n",
       "            0,   66,    2, 1499,    0,  209,    0,    0,  233,  596,    0, 3247,\n",
       "            2,   28,    0,    2,    0,    0,   81,  397],\n",
       "        [1296,    0,    2,    0,    2,   70,    0,    0, 1499,    6,    0,    0,\n",
       "            0, 3870,    0,  615,    0,    2,    0,    0,    2,    2,    0,   39,\n",
       "            0, 3660,    0,    0,    0,    0,  674,  653],\n",
       "        [   9,    0,    0,    0,    0,    6,    0,    0,  249, 2158,    0,    0,\n",
       "            0,    2,    0,  105,    0,    0,    0,    0,    0,    0,    0,  674,\n",
       "            0,   21,    0,    0,    0,    0,    2,    2],\n",
       "        [1142,    0,    0,    0,    0,  332,    0,    0,  238,    2,    0,    0,\n",
       "            0,    0,    0, 2824,    0,    0,    0,    0,    0,    0,    0,    2,\n",
       "            0,  141,    0,    0,    0,    0,    0,    0],\n",
       "        [2530,    0,    0,    0,    0,    2,    0,    0,    6,    0,    0,    0,\n",
       "            0,    0,    0,  512,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,   25,    0,    0,    0,    0,    0,    0],\n",
       "        [   2,    0,    0,    0,    0,    0,    0,    0,  233,    0,    0,    0,\n",
       "            0,    0,    0,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    2,    0,    0,    0,    0,    0,    0],\n",
       "        [   0,    0,    0,    0,    0,    0,    0,    0,    2,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "            0,    0,    0,    0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73fe55a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "appearance_feat = torch.randn(32, 4, 1536)\n",
    "motion_feat = torch.rand(32, 4, 2048)\n",
    "\n",
    "v_feats = encoder(appearance_feat, motion_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f0b832d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 4, 512])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_feats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "52f704e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output, h_lang_lstm, h_attn_lstm = decoder(decoder_input,\n",
    "                                               decoder_hidden_attn,\n",
    "                                               decoder_hidden_lang,\n",
    "                                               v_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c54d7407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5044])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4b3a79b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_hidden_size = 512\n",
    "bottleneck_size = 512\n",
    "feat_size = 1836\n",
    "\n",
    "temporal_attention = TemporalAttention(cfg)\n",
    "\n",
    "last_hidden_attn = torch.randn(32, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2ecf896",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_global_vector, alpha = temporal_attention(last_hidden_attn, v_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "477a7de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 512])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_global_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "680c49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 1\n",
    "batch_size = 32\n",
    "decoder_hidden_size = 512\n",
    "\n",
    "decoder_hidden = torch.zeros(n_layers, \n",
    "                             batch_size,\n",
    "                             decoder_hidden_size)\n",
    "\n",
    "decoder_hidden_attn = (decoder_hidden, decoder_hidden)\n",
    "decoder_hidden_lang = (decoder_hidden, decoder_hidden)\n",
    "\n",
    "decoder_input = torch.LongTensor([[cfg.SOS_token for _ in range(cfg.batch_size)]])\n",
    "appearance_features = torch.randn((32, 28, 512))\n",
    "motion_features = torch.randn((32, 28, 512))\n",
    "\n",
    "v_features = torch.cat((appearance_features, motion_features), dim=-1)\n",
    "\n",
    "v_bar_features = torch.mean(v_features, dim=1, keepdim=True).squeeze(1).unsqueeze(0)\n",
    "\n",
    "embedded = torch.randn((1, 32, 300))\n",
    "\n",
    "input_attn_lstm = torch.cat((v_bar_features, embedded, decoder_hidden_lang[0]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb263fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output, h_lang_lstm, h_attn_lstm = decoder(decoder_input,\n",
    "                                               decoder_hidden_attn,\n",
    "                                               decoder_hidden_lang,\n",
    "                                               v_features\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61dfcf33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5044])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e6d0dcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_attn_lstm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c942bda2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = [1749, 1649, 1549, 1449]\n",
    "target = [628, 234, 76, 18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9838781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['erase', 'puppy', 'denver', 'do']\n",
      "['son', 'rolls', 'several', 'gathered']\n"
     ]
    }
   ],
   "source": [
    "caption = []\n",
    "gt = []\n",
    "\n",
    "for word in output:\n",
    "    caption.append(voc.index2word.get(word))\n",
    "\n",
    "for word in target:\n",
    "    gt.append(voc.index2word.get(word))\n",
    "    \n",
    "print(caption)\n",
    "print(gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab0f2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_features = torch.randn((32, 28, 512))\n",
    "Uh = torch.rand((1, 32, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f639e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = Uh[0].unsqueeze(1).expand_as(v_features)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4d39ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_bar_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d22481a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_lstm = nn.LSTM(input_size=1836, \n",
    "                         hidden_size=512,\n",
    "                         num_layers=1, \n",
    "                         dropout=0.2,\n",
    "                         batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f7e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, h = attention_lstm(input_attn_lstm,\n",
    "                           decoder_hidden_attn)\n",
    "\n",
    "h[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,\n",
    "            inputs, \n",
    "            attn_hidden,\n",
    "            lang_hidden, \n",
    "            v_features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3de8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.cat((v_bar_features, embedded), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8498454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_targets = targets.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2106fa12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac10337",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, (key, value) in enumerate(voc.index2word.items()):\n",
    "    if e < 11:\n",
    "        print(e, key, value)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec7d2d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  77,    6,    6,  144,  280,   28,    6,    6,  140,    6,    4,    6,\n",
       "         144,    6,    6,    6,    6,    6,  144,    6,  255, 2989,    6,    6,\n",
       "         135,  191,    6,    6,  140,  267,    6,    6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
