{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51ce25b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10cb4bed",
   "metadata": {},
   "source": [
    "# ORG-Module\n",
    "\n",
    "Object Relational Graph is a module that learns to describe an object based on its relationship with others in a video. The algorithm consists many steps and stated in the following order:\n",
    "\n",
    "1. Apply pretrained object detector to capture severall class-agnostic proposal.\n",
    "2. The object features is captured on each keyframes.\n",
    "3. The object features then stored in R, where i is the i-th keyframes, and k is the k-th object.\n",
    "4. The number of objects extracted from each frames are five objects.\n",
    "5. The R variable consist of 5 independent object features.\n",
    "6. Define Object Set R K x d, where K is the number of object nodes, and d is the dimension features.\n",
    "7. Define A, where A is a relation coefficient matrix between K nodes.\n",
    "8. Before feeding to A, the R variable is feed to **Fully connected layer** with bias resulting in R'.\n",
    "9. Then A is the product of fully connected layer between R' and R'T\n",
    "10. After that, the product is activated using softmax function and named A^\n",
    "11. Apply the GCN function, R^ = A^ . R . Wr, Where Wr is learnable parameter\n",
    "12. R^ is the enhanced object features with interaction message between objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac4d0bf",
   "metadata": {},
   "source": [
    "# Develop Side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6610f966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the object feats has the dimension of Frames x Objs x features\n",
    "# with batch dimension it becomes 4-D tensor\n",
    "\n",
    "feat_dims = 512\n",
    "k_objects = 5\n",
    "\n",
    "# this means the object is the second object\n",
    "# of the first frame\n",
    "\n",
    "r_obj_feats = torch.rand(k_objects, feat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bc61afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# based on ORG paper A is equal to:\n",
    "# φ(R) . transpose(ψ(R))\n",
    "# where : ...\n",
    "# φ(R) = R . Wi + bi\n",
    "# ψ(R) = R . wj + bj\n",
    "\n",
    "in_features = feat_dims\n",
    "out_features = feat_dims\n",
    "\n",
    "sigma_r = nn.Linear(in_features, out_features)\n",
    "psi_r = nn.Linear(in_features, out_features)\n",
    "a_softmax = nn.Softmax(dim=1)\n",
    "\n",
    "w_r = nn.Linear(in_features, out_features, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ed7e3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_r_out = sigma_r(r_obj_feats)\n",
    "psi_r_out = psi_r(r_obj_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71c75389",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_coeff_mat = torch.matmul(sigma_r_out, torch.t(psi_r_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1333cc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat = a_softmax(a_coeff_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87cf3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_hat_mul_r = torch.matmul(a_hat, r_obj_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f6dfb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = w_r(a_hat_mul_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5178e51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.9213e-02, -7.8978e-02,  7.4341e-02,  ...,  3.1808e-01,\n",
       "          1.4916e-01,  2.8370e-01],\n",
       "        [-1.1797e-01, -1.4182e-01,  4.4578e-02,  ...,  3.4157e-01,\n",
       "          8.7550e-02,  2.5541e-01],\n",
       "        [-1.4583e-01, -8.9589e-02,  2.8276e-04,  ...,  3.0307e-01,\n",
       "          1.1692e-02,  1.8906e-01],\n",
       "        [-1.2058e-01, -6.3393e-02,  8.3677e-02,  ...,  3.1532e-01,\n",
       "          1.4132e-01,  2.3636e-01],\n",
       "        [-6.8676e-02, -1.1910e-01,  7.7551e-02,  ...,  3.4590e-01,\n",
       "          1.5732e-01,  3.2165e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f275acd1",
   "metadata": {},
   "source": [
    "# Class Side (Alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1cb33ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ORG(nn.Module):\n",
    "    \n",
    "    def __init__(self, feat_dims):\n",
    "        super(ORG, self).__init__()\n",
    "        '''\n",
    "        Object Relational Graph (ORG) is a module that learns \n",
    "        to describe an object based on its relationship \n",
    "        with others in a video.\n",
    "        \n",
    "        Arguments:\n",
    "            feat_size : The object feature size that obtained from\n",
    "                        the last fully-connected layer of the backbone\n",
    "                        of Faster R-CNN\n",
    "        '''\n",
    "        \n",
    "        sigma_r = nn.Linear(feat_dims, feat_dims)\n",
    "        psi_r = nn.Linear(feat_dims, feat_dims)\n",
    "        \n",
    "        a_softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "        w_r = nn.Linear(feat_dims, feat_dims, bias=False)\n",
    "        \n",
    "    def forward(self, r_obj_feat):\n",
    "        sigma_r_out = sigma_r(r_obj_feats)\n",
    "        psi_r_out = psi_r(r_obj_feats)\n",
    "        \n",
    "        a_coeff_mat = torch.matmul(sigma_r_out, torch.t(psi_r_out))\n",
    "        a_hat = a_softmax(a_coeff_mat)\n",
    "        \n",
    "        a_hat_mul_r = torch.matmul(a_hat, r_obj_feats)\n",
    "        output = w_r(a_hat_mul_r)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "10bf2bc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "org_module = ORG(feat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb8ed9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-7.9213e-02, -7.8978e-02,  7.4341e-02,  ...,  3.1808e-01,\n",
       "          1.4916e-01,  2.8370e-01],\n",
       "        [-1.1797e-01, -1.4182e-01,  4.4578e-02,  ...,  3.4157e-01,\n",
       "          8.7550e-02,  2.5541e-01],\n",
       "        [-1.4583e-01, -8.9589e-02,  2.8276e-04,  ...,  3.0307e-01,\n",
       "          1.1692e-02,  1.8906e-01],\n",
       "        [-1.2058e-01, -6.3393e-02,  8.3677e-02,  ...,  3.1532e-01,\n",
       "          1.4132e-01,  2.3636e-01],\n",
       "        [-6.8676e-02, -1.1910e-01,  7.7551e-02,  ...,  3.4590e-01,\n",
       "          1.5732e-01,  3.2165e-01]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_hat = org_module(r_obj_feats)\n",
    "r_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bdd6ded",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 512])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d416e4e",
   "metadata": {},
   "source": [
    "# In Practice Using Faster R-CNN Object Features (Beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9418c3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80a05fc2",
   "metadata": {},
   "source": [
    "# Attention LSTM Class (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "26b1cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 hidden_size,\n",
    "                 features_size,\n",
    "                 attn_size):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        '''\n",
    "        Temporal Attention Module of ORG.\n",
    "        It depends on previous hidden state of LSTM attention.\n",
    "        Arguments:\n",
    "          lstm_attn_hidden: The hidden state from LSTM attention\n",
    "                            tensors of shape (batch_size, hidden_size).\n",
    "          video_feats_size: The concatenation of frame features\n",
    "                            and motion features.\n",
    "                            tensors of shape (batch_size, n_frames, feats_size)\n",
    "          attn_size       : The attention size of attention module.\n",
    "        '''\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.features_size = features_size\n",
    "        self.attn_size = attn_size\n",
    "        \n",
    "        # This layer is for the operation between W_a and V_i\n",
    "        # : W_a is the learnable params associated with Video Features\n",
    "        # : V_i is the concatenation between appearance features\n",
    "        #   and motion features\n",
    "        encoder_projection = nn.Linear(features_size, \n",
    "                                       attn_size, \n",
    "                                       bias=False)\n",
    "\n",
    "        # This layer is for the operation between U_a and h_t_attn\n",
    "        # : U_a is the learnable params associated with LSTM attn hidden states\n",
    "        # : h_t_attn is the concatenation between appearance features\n",
    "        #   and motion features\n",
    "        decoder_projection = nn.Linear(hidden_size, \n",
    "                                       attn_size, \n",
    "                                       bias=False)\n",
    "\n",
    "        # This layer is for the operation between w_T and result tanh(W_v + U_h)\n",
    "        # : W_v is the result of matrix multiplication between \n",
    "        #   video features and weight W\n",
    "        # : U_h is the result of matrix multiplication between\n",
    "        #   LSTM attention hidden states and weight U\n",
    "        # : tanh(.) is the tanh activation function\n",
    "        # : w_T is a vector of learnable params for the result\n",
    "        #   of the tanh activation\n",
    "        energy_projection = nn.Linear(attn_size, \n",
    "                                      1, \n",
    "                                      bias=False)\n",
    "        \n",
    "    def forward(self,\n",
    "                h_attn_lstm,\n",
    "                v_features):\n",
    "        '''\n",
    "        shape of hidden attention lstm (batch_size, hidden_size)\n",
    "        shape of video features input (batch_size, n_frames, features_size)\n",
    "        '''\n",
    "        Wv = encoder_projection(v_features)\n",
    "        Uh = decoder_projection(h_attn_lstm)\n",
    "        \n",
    "        Ew = energy_projection(torch.tanh(Wv + Uh))\n",
    "        alpha = softmax_activation(Ew)\n",
    "        \n",
    "        weighted_feats = alpha * v_features\n",
    "        context_global = weighted_feats.sum(dim=1)\n",
    "        \n",
    "        return context_global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "85d866fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 1324\n",
    "hidden_size = 512\n",
    "features_size = 512\n",
    "attn_size = 512\n",
    "num_layers= 1 \n",
    "dropout= 0.5\n",
    "\n",
    "lstm_attn = nn.LSTM(input_size, \n",
    "                    hidden_size, \n",
    "                    num_layers, \n",
    "                    batch_first=True, \n",
    "                    dropout=dropout)\n",
    "\n",
    "temporal_attn = TemporalAttention(hidden_size,\n",
    "                                  features_size,\n",
    "                                  attn_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "1199cbae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 56, 512])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector = torch.randn(1, 28, 512)\n",
    "motion_vector = torch.randn(1, 28, 512)\n",
    "\n",
    "video_features = torch.cat((feature_vector, motion_vector), dim=1)\n",
    "video_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "6f84b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 512])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_bar = torch.mean(video_features, dim=1, keepdim=True)\n",
    "\n",
    "v_bar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "d6ab41aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_word_emb = torch.randn(1, 1, 300)\n",
    "\n",
    "\n",
    "prev_cell_lang_lstm = torch.rand(1, 1, 512)\n",
    "prev_hidden_lang_lstm = torch.rand(1, 1, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "393d2240",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 1324])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_combined = torch.cat((v_bar, prev_word, prev_lang_hidden), dim=-1)\n",
    "prev_h_attn = (prev_hidden_lang_lstm, prev_cell_lang_lstm)\n",
    "\n",
    "input_combined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "1cf1f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs, hidden_attn_lstm = lstm_attn(input_combined)\n",
    "    context_global = temporal_attn(hidden_attn_lstm[0],\n",
    "                                   video_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "61ac5757",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden_lang = hidden_attn_lstm[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "f3f2b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 1\n",
    "\n",
    "last_hidden_lang = last_hidden_lang.view(n_layers, last_hidden_lang.size(1), last_hidden_lang.size(2))\n",
    "# last_hidden_lang = last_hidden_lang[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "17d9a9bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "7f4e62a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = 512\n",
    "output_features = 512\n",
    "\n",
    "# This layer is for the operation between W_a and V_i\n",
    "# : W_a is the learnable params associated with Video Features\n",
    "# : V_i is the concatenation between appearance features\n",
    "#   and motion features\n",
    "encoder_projection = nn.Linear(input_features, \n",
    "                               output_features, \n",
    "                               bias=False)\n",
    "\n",
    "# This layer is for the operation between U_a and h_t_attn\n",
    "# : U_a is the learnable params associated with LSTM attn hidden states\n",
    "# : h_t_attn is the concatenation between appearance features\n",
    "#   and motion features\n",
    "decoder_projection = nn.Linear(input_features, \n",
    "                               output_features, \n",
    "                               bias=False)\n",
    "\n",
    "# This layer is for the operation between w_T and result tanh(W_v + U_h)\n",
    "# : W_v is the result of matrix multiplication between \n",
    "#   video features and weight W\n",
    "# : U_h is the result of matrix multiplication between\n",
    "#   LSTM attention hidden states and weight U\n",
    "# : tanh(.) is the tanh activation function\n",
    "# : w_T is a vector of learnable params for the result\n",
    "#   of the tanh activation\n",
    "energy_projection = nn.Linear(input_features, \n",
    "                              1, \n",
    "                              bias=False)\n",
    "\n",
    "# This layer is for the normalization of all the weights\n",
    "# corresponding to its frame\n",
    "softmax_activation = nn.Softmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "0ad0acf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wv = encoder_projection(video_features)\n",
    "Uh = decoder_projection(h_attn)\n",
    "Ew = energy_projection(torch.tanh(Wv + Uh))\n",
    "alpha = softmax_activation(Ew)\n",
    "weighted_feats = alpha * video_features\n",
    "context_global = weighted_feats.sum(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "9af51b13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_global.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac66495",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 512 \n",
    "hidden_size = 512\n",
    "num_layers= 1 \n",
    "dropout= 0.5\n",
    "\n",
    "attlstm = AttentionLSTM(512,\n",
    "                        512,\n",
    "                        1,\n",
    "                        0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "11291ec7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 2 required positional arguments: 'memory' and 'video_features'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18392/815298799.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattlstm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_combined\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprev_hidden\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1110\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1111\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: forward() missing 2 required positional arguments: 'memory' and 'video_features'"
     ]
    }
   ],
   "source": [
    "output, hidden = attlstm(prev_word, prev_hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da9179",
   "metadata": {},
   "source": [
    "# Temporal Attention (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a209a3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TemporalAttention(nn.Module):\n",
    "    def __init__(self, \n",
    "                 decoder_hidden_size, \n",
    "                 feat_size,\n",
    "                 attn_size,):\n",
    "        super(TemporalAttention, self).__init__()\n",
    "        '''\n",
    "        Temporal Attention module. \n",
    "        It depends on previous hidden memory in the decoder(of shape hidden_size),\n",
    "        feature at the source side ( of shape(196, feat_size) ).  \n",
    "        at(s) = align(ht,hs)\n",
    "              = exp(score(ht,hs)) / Sum(exp(score(ht,hs')))  \n",
    "        where\n",
    "        score(ht,hs) = ht.t * hs                         (dot)\n",
    "                     = ht.t * Wa * hs                  (general)\n",
    "                     = va.t * tanh(Wa[ht;hs])           (concat)  \n",
    "        Here we have used concat formulae.\n",
    "        Argumets:\n",
    "          hidden_size : hidden memory size of decoder. (batch,hidden_size)\n",
    "          feat_size : feature size of each grid (annotation vector) at encoder side.\n",
    "          bottleneck_size : intermediate size.\n",
    "        '''\n",
    "\n",
    "\n",
    "        self.hidden_size = decoder_hidden_size\n",
    "        self.feat_size = feat_size\n",
    "        self.bottleneck_size = attn_size\n",
    "        \n",
    "        self.decoder_projection = nn.Linear(self.hidden_size,\n",
    "                                            self.bottleneck_size,\n",
    "                                            bias=False)\n",
    "        self.encoder_projection = nn.Linear(self.feat_size, \n",
    "                                            self.bottleneck_size, \n",
    "                                            bias=False)\n",
    "        self.final_projection = nn.Linear(self.bottleneck_size, \n",
    "                                          1,\n",
    "                                          bias=False)\n",
    "     \n",
    "    def forward(self, hidden, feats):\n",
    "        '''\n",
    "        shape of hidden (hidden_size) (batch,hidden_size) #(100, 512)\n",
    "        shape of feats (batch size, ,feat_size)  #(100, 40, 1536)\n",
    "        '''\n",
    "\n",
    "        Wh = self.decoder_projection(hidden)  \n",
    "        Uv = self.encoder_projection(feats)   \n",
    "        Wh = Wh.unsqueeze(1).expand_as(Uv)\n",
    "\n",
    "        energies = self.final_projection(torch.tanh(Wh + Uv))\n",
    "\n",
    "        weights = F.softmax(energies, dim=1)\n",
    "        weighted_feats = feats * weights.expand_as(feats)\n",
    "        attn_feats = weighted_feats.sum(dim=1)\n",
    "\n",
    "        return attn_feats, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7ae01d7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size=512\n",
    "bottleneck_size=512\n",
    "feat_size=512\n",
    "\n",
    "decoder_projection = nn.Linear(hidden_size,\n",
    "                               bottleneck_size,\n",
    "                               bias=False)\n",
    "\n",
    "encoder_projection = nn.Linear(feat_size, \n",
    "                               bottleneck_size, \n",
    "                               bias=False)\n",
    "\n",
    "final_projection = nn.Linear(bottleneck_size, \n",
    "                              1,\n",
    "                              bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "84a153bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = torch.randn((100, 512))\n",
    "feats = torch.randn((100, 40, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "087474ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Wh = decoder_projection(hidden)\n",
    "Uv = encoder_projection(feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "baf6a9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = Wh.unsqueeze(1).expand_as(Uv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "bd3e1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = final_projection(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "35d36d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([100, 40, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpha.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "60d41a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights = torch.nn.softmax(alpha, dim=1)\n",
    "weighted_feats = feats * alpha.expand_as(feats)\n",
    "attn_feats = weighted_feats.sum(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186cb5a9",
   "metadata": {},
   "source": [
    "# Spatial Attention (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2a0792",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "321ea80b",
   "metadata": {},
   "source": [
    "# Language LSTM (alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4ea142",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df9c5535",
   "metadata": {},
   "source": [
    "# Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef4d8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from models.ORG_TRL.model import DecoderRNN\n",
    "from config import ConfigORGTRL\n",
    "from config import Path\n",
    "from dictionary import Vocabulary\n",
    "from config import Path\n",
    "from data import DataHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9b6db5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.4 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 28, 1536])\n",
      "torch.Size([24, 32])\n",
      "torch.Size([24, 32])\n",
      "24\n",
      "torch.Size([32, 28, 2048])\n"
     ]
    }
   ],
   "source": [
    "cfg = ConfigORGTRL(opt_encoder=True)\n",
    "# specifying the dataset in configuration object from {'msvd','msrvtt'}\n",
    "cfg.dataset = 'msrvtt'\n",
    "\n",
    "voc = Vocabulary(cfg, gloVe=True)\n",
    "path = Path(cfg, os.getcwd())\n",
    "voc.load()\n",
    "\n",
    "decoder = DecoderRNN(cfg, voc)\n",
    "\n",
    "# data_handler = DataHandler(cfg, path, voc)\n",
    "# train_dset, val_dset, test_dset = data_handler.getDatasets()\n",
    "# train_loader, val_loader, test_loader = data_handler.getDataloader(train_dset, val_dset, test_dset)\n",
    "\n",
    "# for data in train_loader:\n",
    "#     appearance_features, targets, mask, max_length, _, motion_features, _ = data\n",
    "#     print(appearance_features.shape)\n",
    "#     print(targets.shape)\n",
    "#     print(mask.shape)\n",
    "#     print(max_length)\n",
    "#     print(motion_features.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "680c49af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_layers = 1\n",
    "batch_size = 32\n",
    "decoder_hidden_size = 512\n",
    "\n",
    "decoder_hidden = torch.zeros(n_layers, \n",
    "                             batch_size,\n",
    "                             decoder_hidden_size)\n",
    "\n",
    "decoder_hidden_attn = (decoder_hidden, decoder_hidden)\n",
    "decoder_hidden_lang = (decoder_hidden, decoder_hidden)\n",
    "\n",
    "decoder_input = torch.LongTensor([[cfg.SOS_token for _ in range(cfg.batch_size)]])\n",
    "appearance_features = torch.randn((32, 28, 512))\n",
    "motion_features = torch.randn((32, 28, 512))\n",
    "\n",
    "v_features = torch.cat((appearance_features, motion_features), dim=-1)\n",
    "\n",
    "v_bar_features = torch.mean(v_features, dim=1, keepdim=True).squeeze(1).unsqueeze(0)\n",
    "\n",
    "embedded = torch.randn((1, 32, 300))\n",
    "\n",
    "input_attn_lstm = torch.cat((v_bar_features, embedded, decoder_hidden_lang[0]), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb263fbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    output, h_lang_lstm, h_attn_lstm = decoder(decoder_input,\n",
    "                                               decoder_hidden_attn,\n",
    "                                               decoder_hidden_lang,\n",
    "                                               v_features\n",
    "                                               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e81fd740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 5044])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2bb8516",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_attn_lstm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab0f2137",
   "metadata": {},
   "outputs": [],
   "source": [
    "v_features = torch.randn((32, 28, 512))\n",
    "Uh = torch.rand((1, 32, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f639e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 28, 512])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = Uh[0].unsqueeze(1).expand_as(v_features)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4d39ddc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 1024])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v_bar_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d22481a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_lstm = nn.LSTM(input_size=1836, \n",
    "                         hidden_size=512,\n",
    "                         num_layers=1, \n",
    "                         dropout=0.2,\n",
    "                         batch_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d9f7e070",
   "metadata": {},
   "outputs": [],
   "source": [
    "output, h = attention_lstm(input_attn_lstm,\n",
    "                           decoder_hidden_attn)\n",
    "\n",
    "h[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044e95f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,\n",
    "            inputs, \n",
    "            attn_hidden,\n",
    "            lang_hidden, \n",
    "            v_features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f3de8f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = torch.cat((v_bar_features, embedded), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8498454a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_targets = targets.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2106fa12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 23])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_targets.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac10337",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e, (key, value) in enumerate(voc.index2word.items()):\n",
    "    if e < 11:\n",
    "        print(e, key, value)\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec7d2d18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  77,    6,    6,  144,  280,   28,    6,    6,  140,    6,    4,    6,\n",
       "         144,    6,    6,    6,    6,    6,  144,    6,  255, 2989,    6,    6,\n",
       "         135,  191,    6,    6,  140,  267,    6,    6])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
