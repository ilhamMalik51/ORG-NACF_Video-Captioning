{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "197146cd",
   "metadata": {},
   "source": [
    "# This Is The Teacher Reinforcement Learning Development Section"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b2e8de",
   "metadata": {},
   "source": [
    "# Alur Fix ELM BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce72818",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from utils import Utils\n",
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ed4426",
   "metadata": {},
   "source": [
    "### Inisialisasi Target Untuk Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8cba106",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [[   6,  255,   77,    6,  140,    6,    6,    6,    6,    6,    6,    6,\n",
    "            6,   77,   92,    6,    6,    6,    6,  135,    6,    6,  451,   11,\n",
    "            6,    9, 2708,    6,    6,  353,    6, 3782],\n",
    "        [  14,   25,   25,   92,   25,   92,  263,   85,  202,   92,  525,   79,\n",
    "         1649,   25,  462,  469,   61, 1340,   92,  187,  165,  263,    6,   21,\n",
    "         2303,   28, 2707,  219,   87,  167,  630, 3780],\n",
    "        [  15,   34,  255,   25,    6,   25,  629,   86,  249,  283,  330,  283,\n",
    "           25,    6,   70,   92,   25,   79,  283,  195,  299, 1117,  187,   22,\n",
    "           25,  996,    5,   34,  103,  748,    6,   25],\n",
    "        [  53,   99, 3622,   97,   99, 1428,    6,   87, 2016,   12,  235,  748,\n",
    "          607,   92,  243,   34,  624,   25,  198,   17,  247, 1115,   99,  499,\n",
    "          352,   28,   28,  117, 1850,   28, 2462,  579],\n",
    "        [  21,  206,   39,    2,   12,   39,  519,    2,    6, 2383,   28,    6,\n",
    "            2,  631,    6,    6,    2,  117,   28,    2,   66, 1118,    2,   21,\n",
    "           70, 1244, 2110,   70, 3132,  932,  537,   39],\n",
    "        [ 345,    2,    6,    0,   28,    6,    2,    0, 1535, 2366, 2078,   99,\n",
    "            0,    6,   99,  206,    0,   12,  118,    0,    6,   70,    0,  332,\n",
    "            6,   25,    2,   28,    2,    2,  446,   28],\n",
    "        [ 671,    0, 1510,    0, 3811,   99,    0,    0,   21, 2379,    2,  206,\n",
    "            0,  478,  206,   15,    0,  139,    2,    0,  202,  284,    0,   66,\n",
    "           92, 2306,    0, 1154,    0,    0,   39,  589],\n",
    "        [  21,    0,  429,    0, 3723,  336,    0,    0,    6,    5,    0,    2,\n",
    "            0,   66,    2, 1499,    0,  209,    0,    0,  233,  596,    0, 3247,\n",
    "            2,   28,    0,    2,    0,    0,   81,  397],\n",
    "        [1296,    0,    2,    0,    2,   70,    0,    0, 1499,    6,    0,    0,\n",
    "            0, 3870,    0,  615,    0,    2,    0,    0,    2,    2,    0,   39,\n",
    "            0, 3660,    0,    0,    0,    0,  674,  653],\n",
    "        [   9,    0,    0,    0,    0,    6,    0,    0,  249, 2158,    0,    0,\n",
    "            0,    2,    0,  105,    0,    0,    0,    0,    0,    0,    0,  674,\n",
    "            0,   21,    0,    0,    0,    0,    2,    2],\n",
    "        [1142,    0,    0,    0,    0,  332,    0,    0,  238,    2,    0,    0,\n",
    "            0,    0,    0, 2824,    0,    0,    0,    0,    0,    0,    0,    2,\n",
    "            0,  141,    0,    0,    0,    0,    0,    0],\n",
    "        [2530,    0,    0,    0,    0,    2,    0,    0,    6,    0,    0,    0,\n",
    "            0,    0,    0,  512,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,   25,    0,    0,    0,    0,    0,    0],\n",
    "        [   2,    0,    0,    0,    0,    0,    0,    0,  233,    0,    0,    0,\n",
    "            0,    0,    0,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    2,    0,    0,    0,    0,    0,    0],\n",
    "        [   0,    0,    0,    0,    0,    0,    0,    0,    2,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e46258",
   "metadata": {},
   "source": [
    "### Loading Bert model yang sudah di-fine tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c082bce",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_ft_path = \"Saved\\\\bert_finetuned_1000_data_fix.pt\"\n",
    "\n",
    "# Instantiate the model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "# Assign the saved state dictionary to the model's state dictionary\n",
    "model.load_state_dict(torch.load(bert_ft_path, map_location=torch.device('cpu')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ef5b4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2b = \"Saved\\\\msrvtt_index2bert_dic.p\"\n",
    "b2i = \"Saved\\\\msrvtt_bert2index_dic.p\"\n",
    "\n",
    "with open(i2b, 'rb') as fp:\n",
    "    index2bert = pickle.load(fp)\n",
    "    \n",
    "with open(b2i, 'rb') as fp:\n",
    "    bert2index = pickle.load(fp)\n",
    "    \n",
    "targets = torch.tensor(target)\n",
    "decoder_output = torch.rand(32, 5044)\n",
    "\n",
    "index2bert_ts = torch.load('Saved\\\\index2bert_ts.pt').long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be936dbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(index2bert_ts.requires_grad_(False)).requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98840583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "temperature = 3\n",
    "\n",
    "bert_targets = index2bert_ts[targets.T].detach()\n",
    "\n",
    "for t in range(1):\n",
    "    # this iteration will be the decoding steps\n",
    "    # and the input into the bert model\n",
    "    inputs_id = bert_targets.detach()\n",
    "    inputs_id[:, t][inputs_id[:, t] != 0] = 103\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bert_output = model(inputs_id)[0]\n",
    "        bert_output = torch.exp(F.log_softmax((bert_output / temperature), dim=-1))\n",
    "    \n",
    "    topk_bert_probs, topk_bert_indices = torch.topk(bert_output[:, t, :], k=50, dim=-1)\n",
    "    topk_bert_indices[topk_bert_indices == 100] = 0 # untuk filter unknown word menjadi padding\n",
    "    \n",
    "    topk_indices = torch.full_like(topk_bert_indices, 3)\n",
    "    output_tuple = (torch.eq(index2bert_ts.view(1, -1), topk_bert_indices.unsqueeze(-1))).nonzero(as_tuple=True)\n",
    "    topk_indices[output_tuple[0], output_tuple[1]] = output_tuple[2]\n",
    "    \n",
    "    mask = torch.ones_like(topk_indices)\n",
    "    # 3 adalah Unknown Token dan 0 adalah Padding Token\n",
    "    mask[(topk_indices.eq(3) | topk_indices.eq(0))] = 0 \n",
    "    \n",
    "    kl_loss = utils.KLLoss(torch.gather(decoder_output, 1, topk_indices), \n",
    "                           topk_bert_probs,\n",
    "                           mask, \n",
    "                           t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d3d4ece6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0056, 0.0005, 0.0005, 0.0004, 0.0003, 0.0003, 0.0003, 0.0003, 0.0003,\n",
       "        0.0003, 0.0003, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "        0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002, 0.0002,\n",
       "        0.0002, 0.0002, 0.0002, 0.0002, 0.0001])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_bert_probs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bf1e0e26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0504, 0.6130, 0.9854,  ..., 0.5427, 0.2433, 0.2433],\n",
       "        [0.5275, 0.1613, 0.5181,  ..., 0.6169, 0.6169, 0.6169],\n",
       "        [0.7475, 0.6516, 0.5980,  ..., 0.6445, 0.2859, 0.2859],\n",
       "        ...,\n",
       "        [0.3278, 0.6598, 0.6856,  ..., 0.0877, 0.0196, 0.1323],\n",
       "        [0.7448, 0.3362, 0.4800,  ..., 0.8062, 0.6772, 0.0304],\n",
       "        [0.2889, 0.3607, 0.0589,  ..., 0.3500, 0.3500, 0.4628]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.gather(decoder_output, 1, topk_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02c9ce6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index out of range in self",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_572/355049617.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtopk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecoder_output\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   2181\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2182\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2183\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index out of range in self"
     ]
    }
   ],
   "source": [
    "F.embedding(topk_indices, decoder_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf8fa9a",
   "metadata": {},
   "source": [
    "# Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f927ba28",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_indices = torch.tensor(np.frompyfunc(\n",
    "                    lambda x: self.voc.bert2index.get(x, 3), 1, 1)(topk_bert_indices.detach().cpu().numpy()).astype('int32')).long().to(self.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ad94806a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (topk_bert_indices == index2bert_ts.unsqueeze(-1)).nonzero(as_tuple=False)\n",
    "\n",
    "topk_indices_ts = torch.full((32, 50), 3).long()\n",
    "for i in range(32):\n",
    "    indices = (torch.eq(index2bert_ts, topk_bert_indices[i].view(-1, 1))).nonzero(as_tuple=True)[1]\n",
    "    topk_indices_ts[i][:indices.size(0)] = indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "a6e0bd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "topk_indices_ts = torch.full((32, 50), 3).long()\n",
    "output_tuple = (torch.eq(index2bert_ts.unsqueeze(0), topk_bert_indices.unsqueeze(-1))).nonzero(as_tuple=True)\n",
    "topk_indices_ts[output_tuple[0], output_tuple[1]] = output_tuple[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "926a76a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0]),)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_indices[1][bert_indices[1] == 100].nonzero(as_tuple=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "12c0135a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([5]), tensor([43]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(bert_indices[1] == 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3aeb0af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2045,  2023,     0,  2009,  2002,  2182,  2008,  2016,  2029,  2054,\n",
       "        12397,  8333,  2028,  2169,  2673,  2619,  2242,  3491,  1998, 12528,\n",
       "        28137,  2166,  2073,  3793,  3256,  2059,  6662,  1610,  2678,  2178,\n",
       "         2592,  2279, 21021, 18558, 17716,  2051,  2974, 12115, 25439,  2833,\n",
       "         2061,  3967, 11247, 19769,  2600,  3941,  2946,  5789, 16967,  7284])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_indices[1][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4f212154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([], dtype=torch.int64)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_indices[0][vocab_indices[0]==2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "f6e5a477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor(0.0407), tensor(0.0125), tensor(0.0064)]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "153d51da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['EOS', 'mixing', 'group', 'UNK', 'mixture', 'groups', 'band', 'consisting', 'mixes', 'composed', 'bands', 'UNK', 'mix', 'UNK', 'music', 'UNK', 'each', 'scores', 'UNK', 'together', 'UNK', 'UNK', 'UNK', 'pool', 'UNK', 'UNK', 'lots', 'UNK', 'pools', 'UNK', 'UNK', 'UNK', 'combining', 'UNK', 'UNK', 'UNK', 'footage', 'conference', 'UNK', 'UNK', 'mixed', 'UNK', 'simultaneously', 'UNK', 'dance', 'number', 'teams', 'UNK', 'UNK', 'lists']\n"
     ]
    }
   ],
   "source": [
    "print([index2word.get(tok_id) for tok_id in vocab_indices[1][0].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "43289da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', 'mixing', 'group', 'combination', 'mixture', 'groups', 'band', 'consisting', 'mixes', 'composed', 'bands', 'alliance', 'mix', 'blending', 'music', 'consists', 'each', 'scores', 'consist', 'together', 'consisted', 'majority', 'pairing', 'pool', 'confluence', 'association', 'lots', 'ensembles', 'pools', 'combinations', 'grouping', 'amalgamation', 'combining', 'composite', 'coalition', 'separation', 'footage', 'conference', 'equality', 'rivalry', 'mixed', 'celaena', 'simultaneously', 'choirs', 'dance', 'number', 'teams', 'presence', 'mingled', 'lists']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens(bert_indices[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "13cfc812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   6,   28,  105,   21,  451,   39,  362,  140,  418,  162,  248,   36,\n",
       "          66,  601,   59,   33,    5,   70,   81,  531,  177,  459,  144,   77,\n",
       "         135, 1774,  139,  110,  328,  216,  154,   15,  469, 2831,  103,  295,\n",
       "        3152,  430, 1269,   76,  355, 1603,  258,  990,  646,  460,  864,  179,\n",
       "         776,  672])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4ce002de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_ids_to_tokens(topk_bert_indices[0]) == [index2word.get(tok_id) for tok_id in topk_indices_ts[0].detach().numpy()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "eaf94857",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bowls', 'still', 'now', 'casually', 'he', 'television', 'profile', 'all', 'president', 'his', 'mark', 'tony', 'binoculars', 'while', 'chris', 'outdoors', 'fell', 'trek', 'hanging', 'these', 'pictures', 'PAD', 'EOS', 'tonight', 'looking', 'here', 'this', 'towards', 'slowly', 'you', '-', 'displaying', 'though', 'resting', 'mike', 'already', 'there', 'two', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK', 'UNK']\n"
     ]
    }
   ],
   "source": [
    "print([index2word.get(tok_id) for tok_id in topk_indices_ts[3].detach().numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b38759de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1037, 1996, 2019, 1998, 2049, 1999, 2214, 2023, 2189, 2028, 2658, 2037,\n",
       "        2007, 3080, 2235, 2304, 2006, 2000, 2010, 2047, 4854, 2312, 2048, 2045,\n",
       "        2070, 2590, 2178, 2059, 2041, 2009, 2362, 1997, 2402, 2880, 2005, 2004,\n",
       "        2021, 2367, 2204, 2195, 2502, 2026, 2008, 2122, 4748, 2119, 2307, 2013,\n",
       "        2601, 3315])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index2bert_ts[torch.where(index2bert_ts == topk_bert_indices[0].unsqueeze(-1))[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "54b5ef81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1037, 1996, 2019, 1998, 2049, 1999, 2214, 2023, 2189, 2028, 2658, 2037,\n",
       "        2007, 3080, 2235, 2304, 2006, 2000, 2010, 2047, 4854, 2312, 2048, 2045,\n",
       "        2070, 2590, 2178, 2059, 2041, 2009, 2362, 1997, 2402, 2880, 2005, 2004,\n",
       "        2021, 2367, 2204, 2195, 2502, 2026, 2008, 2122, 4748, 2119, 2307, 2013,\n",
       "        2601, 3315])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_bert_indices[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "584baf1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# index di index2bert merupakan\n",
    "# index yang ada di my custom vocabulary\n",
    "\n",
    "# sedangkan value index2bert_ts adalah\n",
    "# bert index\n",
    "\n",
    "# inisialisasi dimana semua value adalah UNK Token\n",
    "index2bert_ts = torch.full((5045,), 100).long()\n",
    "\n",
    "# value diisi dengan token yang diketahui\n",
    "for k, v in index2bert.items():\n",
    "    index2bert_ts[k] = v\n",
    "\n",
    "# indeks kedua diisi dengan padding\n",
    "index2bert_ts[2] = 0\n",
    "torch.save(index2bert_ts, 'Saved\\\\index2bert_ts.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314a990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(32):\n",
    "    print(bert_targets_np[i] == bert_targets[i].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "9a04d16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2bert_ts = torch.load('Saved\\\\index2bert_ts.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f818ab",
   "metadata": {},
   "source": [
    "# CHECK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fc40c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLLoss(decoder_output, bert_output, topk_indices, topk_bert_indices, t):\n",
    "    UNK_TOKEN_ID = 3\n",
    "    PAD_TOKEN_ID = 0\n",
    "    mask = torch.ones_like(topk_indices)\n",
    "    mask[(topk_indices.eq(UNK_TOKEN_ID) | topk_indices.eq(PAD_TOKEN_ID))] = 0\n",
    "    \n",
    "    log_loss_p = torch.log(torch.gather(decoder_output, 1, topk_indices))\n",
    "    log_loss_p = log_loss_p * mask\n",
    "    proba_q = torch.gather(bert_output[:, t], 1, topk_bert_indices)\n",
    "    kl_loss = (-(proba_q * log_loss_p).sum(dim=-1)).mean()\n",
    "    \n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c1e949b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0140)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KLLoss(decoder_output, bert_output_ls[1], vocab_indices[1], bert_indices[1], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "89e28d0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0129)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5cfe9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_var = 0.3\n",
    "mask_loss = torch.tensor([0.78879])\n",
    "loss = 0\n",
    "loss += (((1 - lambda_var) * mask_loss) + (lambda_var * kl_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0038b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to pre-defined vocabulary file\n",
    "vocab_file_path = \"Saved\\msrvtt_index2word_dic.p\"\n",
    "w2i = \"Saved\\msrvtt_word2index_dic.p\"\n",
    "i2w = \"Saved\\msrvtt_index2word_dic.p\"\n",
    "\n",
    "# load pre-defined vocabulary file\n",
    "with open(vocab_file_path, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "with open(w2i, \"rb\") as f:\n",
    "    word2index = pickle.load(f)  \n",
    "\n",
    "with open(i2w, \"rb\") as f:\n",
    "    index2word = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "041e5d5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f2e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "[index2word.get(token_id) for token_id in targets.T[1].detach().cpu().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b23d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "[index2word.get(token_id) for token_id in vocab_indices[1][0].detach().cpu().tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b94c1b6",
   "metadata": {},
   "source": [
    "## Fine-tuning BERT Model Using MSR-VTT Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f644bbe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "from transformers import BertForMaskedLM, BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721e4254",
   "metadata": {},
   "source": [
    "## Preparation video id yang digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1881d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "path = \"MSRVTT\\\\captions\\\\train_val_videodatainfo.json\"\n",
    "\n",
    "with open(path, 'r') as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "train_id_list = [i for i in range(0, 850)]\n",
    "val_id_list = [i for i in range(850, 900)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc23b931",
   "metadata": {},
   "source": [
    "## Membangun vocab untuk di-load Bert Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81faa083",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to pre-defined vocabulary file\n",
    "vocab_file_path = \"Saved\\msrvtt_index2word_dic.p\"\n",
    "w2i = \"Saved\\msrvtt_word2index_dic.p\"\n",
    "i2w = \"Saved\\msrvtt_index2word_dic.p\"\n",
    "my_vocab_path = \"Saved\\my_vocab.txt\"\n",
    "\n",
    "# load pre-defined vocabulary file\n",
    "with open(vocab_file_path, \"rb\") as f:\n",
    "    vocab = pickle.load(f)\n",
    "\n",
    "with open(w2i, \"rb\") as f:\n",
    "    word2index = pickle.load(f)  \n",
    "\n",
    "with open(i2w, \"rb\") as f:\n",
    "    index2word = pickle.load(f)\n",
    "    \n",
    "# with open(my_vocab_path, 'w') as file:\n",
    "#     for _, v in vocab.items():\n",
    "#         file.write(v + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95bef9c0",
   "metadata": {},
   "source": [
    "## Inisialisasi Bert Tokenizer menggunakan metode vocab yang sama dengan metode video cpationing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3db3647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained BERT model and tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "my_vocab_path = \"Saved\\my_vocab.txt\"\n",
    "my_tokenizer = BertTokenizer(vocab_file=my_vocab_path, pad_token=\"PAD\", unk_token=\"UNK\")\n",
    "# len(my_tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a985a8",
   "metadata": {},
   "source": [
    "## Memotong Vocab yang Digunakan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a69e0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocab and store it in list type\n",
    "tokens = list(tokenizer.vocab.keys())\n",
    "my_vocab = list(vocab.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4aa2b79d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexes = [0, 1, 2, 3]\n",
    "values = ['[PAD]', '[CLS]', '[SEP]', '[UNK]']\n",
    "\n",
    "for (index, replacement) in zip(indexes, values):\n",
    "    my_vocab[index] = replacement\n",
    "\n",
    "my_vocab.append(\"[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa62b1c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [tokenizer.vocab[token] for token in my_vocab if token in tokens]\n",
    "my_tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', vocab_size=len(indices), unk_token='[UNK]')\n",
    "my_tokenizer.vocab = {k: v for k, v in tokenizer.vocab.items() if v in indices}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "892a0423",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4335"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba16d089",
   "metadata": {},
   "source": [
    "## Menghubungkan Token IDs Methode Tokenizer CAP dan BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7cea7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "index2bert = {}\n",
    "bert2index = {}\n",
    "\n",
    "for i, token in enumerate(my_vocab):\n",
    "    if token in tokens:\n",
    "        index2bert[i] = tokenizer.vocab[token]\n",
    "        bert2index[tokenizer.vocab[token]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "51b9980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2b = \"Saved\\\\index2bert_dic.p\"\n",
    "b2i = \"Saved\\\\bert2index_dic.p\"\n",
    "\n",
    "with open(i2b, 'wb') as fp:\n",
    "    pickle.dump(index2bert, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open(b2i, 'wb') as fp:\n",
    "    pickle.dump(bert2index, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446f3f67",
   "metadata": {},
   "source": [
    "## Melakukan Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "aa9be081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_caption(data, id_list): \n",
    "    encoded_captions = []\n",
    "\n",
    "    for caption in data['sentences']:\n",
    "        if int(caption['video_id'][5:]) in id_list:\n",
    "            encoded_caption = tokenizer.encode_plus(\n",
    "                caption['caption'],\n",
    "                padding='max_length',\n",
    "                max_length=128,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            encoded_captions.append(encoded_caption)\n",
    "        \n",
    "    return encoded_captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "932a43f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_captions = encode_caption(data, train_id_list)\n",
    "val_encoded_captions = encode_caption(data, val_id_list)\n",
    "\n",
    "torch.save(encoded_captions, 'MSRVTT\\\\captions\\\\encoded_captions_for_bert.pt')\n",
    "torch.save(val_encoded_captions, 'MSRVTT\\\\captions\\\\val_encoded_captions_for_bert.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c3ee7d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_path = 'MSRVTT\\\\captions\\\\encoded_captions_for_bert.pt'\n",
    "val_encoded_path = 'MSRVTT\\\\captions\\\\val_encoded_captions_for_bert.pt'\n",
    "\n",
    "encoded_captions = torch.load(encoded_path)\n",
    "vaval_encoded_captions = torch.load(val_encoded_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff49f4",
   "metadata": {},
   "source": [
    "\n",
    "## Preparing For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6039dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3831394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(encoded_captions, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_encoded_captions, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbb4441",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the fine-tuned BERT model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.train()\n",
    "\n",
    "# Define the optimizer and learning rate\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model on the caption data\n",
    "for epoch in range(10):\n",
    "    for data in train_loader:\n",
    "        input_ids = data['input_ids'].squeeze(1)\n",
    "        attention_mask = data['attention_mask'].squeeze(1)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Evaluate the model on the validation set\n",
    "    with torch.no_grad():\n",
    "        val_loss = 0.0\n",
    "        for data in val_loader:\n",
    "            input_ids = data['input_ids'].squeeze(1)\n",
    "            attention_mask = data['attention_mask'].squeeze(1)\n",
    "        \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "            val_loss += outputs.loss.item() * input_ids.shape[0]\n",
    "        val_loss /= len(val_dataset)\n",
    "    \n",
    "    print(f'Epoch {epoch+1}: Train Loss = {loss.item():.3f}, Val Loss = {val_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "260a2b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "for data in train_loader:\n",
    "    print(data['attention_mask'].squeeze(1).shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebd8aa56",
   "metadata": {},
   "source": [
    "# Training Algorithm For Base-TRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96cd6a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = [[   6,  255,   77,    6,  140,    6,    6,    6,    6,    6,    6,    6,\n",
    "            6,   77,   92,    6,    6,    6,    6,  135,    6,    6,  451,   11,\n",
    "            6,    9, 2708,    6,    6,  353,    6, 3782],\n",
    "        [  14,   25,   25,   92,   25,   92,  263,   85,  202,   92,  525,   79,\n",
    "         1649,   25,  462,  469,   61, 1340,   92,  187,  165,  263,    6,   21,\n",
    "         2303,   28, 2707,  219,   87,  167,  630, 3780],\n",
    "        [  15,   34,  255,   25,    6,   25,  629,   86,  249,  283,  330,  283,\n",
    "           25,    6,   70,   92,   25,   79,  283,  195,  299, 1117,  187,   22,\n",
    "           25,  996,    5,   34,  103,  748,    6,   25],\n",
    "        [  53,   99, 3622,   97,   99, 1428,    6,   87, 2016,   12,  235,  748,\n",
    "          607,   92,  243,   34,  624,   25,  198,   17,  247, 1115,   99,  499,\n",
    "          352,   28,   28,  117, 1850,   28, 2462,  579],\n",
    "        [  21,  206,   39,    2,   12,   39,  519,    2,    6, 2383,   28,    6,\n",
    "            2,  631,    6,    6,    2,  117,   28,    2,   66, 1118,    2,   21,\n",
    "           70, 1244, 2110,   70, 3132,  932,  537,   39],\n",
    "        [ 345,    2,    6,    0,   28,    6,    2,    0, 1535, 2366, 2078,   99,\n",
    "            0,    6,   99,  206,    0,   12,  118,    0,    6,   70,    0,  332,\n",
    "            6,   25,    2,   28,    2,    2,  446,   28],\n",
    "        [ 671,    0, 1510,    0, 3811,   99,    0,    0,   21, 2379,    2,  206,\n",
    "            0,  478,  206,   15,    0,  139,    2,    0,  202,  284,    0,   66,\n",
    "           92, 2306,    0, 1154,    0,    0,   39,  589],\n",
    "        [  21,    0,  429,    0, 3723,  336,    0,    0,    6,    5,    0,    2,\n",
    "            0,   66,    2, 1499,    0,  209,    0,    0,  233,  596,    0, 3247,\n",
    "            2,   28,    0,    2,    0,    0,   81,  397],\n",
    "        [1296,    0,    2,    0,    2,   70,    0,    0, 1499,    6,    0,    0,\n",
    "            0, 3870,    0,  615,    0,    2,    0,    0,    2,    2,    0,   39,\n",
    "            0, 3660,    0,    0,    0,    0,  674,  653],\n",
    "        [   9,    0,    0,    0,    0,    6,    0,    0,  249, 2158,    0,    0,\n",
    "            0,    2,    0,  105,    0,    0,    0,    0,    0,    0,    0,  674,\n",
    "            0,   21,    0,    0,    0,    0,    2,    2],\n",
    "        [1142,    0,    0,    0,    0,  332,    0,    0,  238,    2,    0,    0,\n",
    "            0,    0,    0, 2824,    0,    0,    0,    0,    0,    0,    0,    2,\n",
    "            0,  141,    0,    0,    0,    0,    0,    0],\n",
    "        [2530,    0,    0,    0,    0,    2,    0,    0,    6,    0,    0,    0,\n",
    "            0,    0,    0,  512,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,   25,    0,    0,    0,    0,    0,    0],\n",
    "        [   2,    0,    0,    0,    0,    0,    0,    0,  233,    0,    0,    0,\n",
    "            0,    0,    0,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    2,    0,    0,    0,    0,    0,    0],\n",
    "        [   0,    0,    0,    0,    0,    0,    0,    0,    2,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
    "            0,    0,    0,    0,    0,    0,    0,    0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edc24cc",
   "metadata": {},
   "source": [
    "## Loss Cross Entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83118fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "373f0b03",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertForMaskedLM' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4268/1456360591.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0msaved_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbert_ft_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'cpu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Instantiate the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBertForMaskedLM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'bert-base-uncased'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;31m# Assign the saved state dictionary to the model's state dictionary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msaved_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BertForMaskedLM' is not defined"
     ]
    }
   ],
   "source": [
    "bert_ft_path = \"Saved\\\\bert_finetuned_1000_data_fix.pt\"\n",
    "\n",
    "saved_state = torch.load(bert_ft_path, map_location=torch.device('cpu'))\n",
    "# Instantiate the model\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "# Assign the saved state dictionary to the model's state dictionary\n",
    "model.load_state_dict(saved_state)\n",
    "\n",
    "# my_vocab_path = \"Saved\\my_vocab.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95cd6cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a22932c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "i2b = \"Saved\\\\index2bert_dic.p\"\n",
    "b2i = \"Saved\\\\bert2index_dic.p\"\n",
    "\n",
    "with open(i2b, 'rb') as fp:\n",
    "    index2bert = pickle.load(fp)\n",
    "    \n",
    "with open(b2i, 'rb') as fp:\n",
    "    bert2index = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5345c47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = targets[2].view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c95da857",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ts = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "30d815bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 14])"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_ts.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "f84d46eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_index(t):\n",
    "    return torch.tensor([index2bert.get(dic_idx, 100) for dic_idx in t.squeeze(1).cpu().tolist()]).long()\n",
    "\n",
    "def get_token_index(bert_indices, bert2index):\n",
    "    return torch.tensor([[bert2index.get(bert_id, 3) for bert_id in bert_indices[i].cpu().tolist()] for i in range(bert_indices.size(0))]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "4584dae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_t = get_bert_index(t)\n",
    "bert_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "74190eca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   6,  255,   77,    6,  140,    6,    6,    6,    6,    6,    6,    6,\n",
      "           6,   77,   92,    6,    6,    6,    6,  135,    6,    6,  451,   11,\n",
      "           6,    9, 2708,    6,    6,  353,    6, 3782])\n"
     ]
    }
   ],
   "source": [
    "max_length = 28\n",
    "\n",
    "for t in range(max_length):\n",
    "    print(targets[t])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "4dd5c095",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_variable = targets.T\n",
    "for t in range(targets.size(0)):\n",
    "    if t != 0:\n",
    "        for j in range(targets.size(1)):\n",
    "            if target_variable[j][t] != 0:\n",
    "                target_variable[j][t] = 103\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "1ed25604",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_t = get_bert_index(targets[0].view(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1178fb",
   "metadata": {},
   "source": [
    "## My Algorithm Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "e5992919",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs = targets.T.detach().cpu().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850d09c2",
   "metadata": {},
   "source": [
    "## Minot Improvement Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "8b489b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_targets = torch.tensor([[index2bert.get(token_id.item(), 100) for token_id in batch] for batch in targets.T])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886e69bb",
   "metadata": {},
   "source": [
    "## There is issue in using np.vectorize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "id": "a60226d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the '32' number to cfg.batch_size for the later implementation\n",
    "bert_targets_np = np.hstack([np.array([[101]] * 32), np.vectorize(index2bert.get)(targets.T.detach().cpu().numpy(), 100)])\n",
    "bert_targets = torch.tensor(bert_targets_np)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f3e4e96",
   "metadata": {},
   "source": [
    "## Try using lambda ufunc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "id": "7ddc9499",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_targets_np = np.hstack([np.array([[101]] * 32), np.frompyfunc(lambda x: index2bert.get(x, 100), 1, 1)(targets.T.detach().cpu().numpy())]).astype(np.int64)\n",
    "bert_targets = torch.tensor(bert_targets_np).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcaaad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  '[PAD]': 0,\n",
    "#  '[UNK]': 100,\n",
    "#  '[CLS]': 101,\n",
    "#  '[SEP]': 102,\n",
    "#  '[MASK]': 103,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "65ed9698",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 6, 14, 15, 53, 21, 345, 671, 21, 1296, 9, 1142, 2530, 2, 0]"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[101] + bert_inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "26d05906",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_bert_inputs = []\n",
    "\n",
    "for i in range(targets.T.size(0)):\n",
    "    temp = [101]\n",
    "    for j in range(targets.T.size(1)):\n",
    "        temp.append(index2bert.get(bert_inputs[i][j]))\n",
    "    \n",
    "    new_bert_inputs.append(temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3f7557",
   "metadata": {},
   "source": [
    "## Function for preparing Bert Inputs\n",
    "\n",
    "Jadi fungsi ini digunakan untuk merubah token IDs dari target variable menjadi Bert Token IDs serta menambahkan [CLS] token di awal sentence. Hal ini juga dilakukan agar inference Bert Optimal dimana Bert perlu menerima seluruh kalimat dan menambah token [MASK] pada posisi tertentu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7222f72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "2cfd0df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_inputs_ls = [[101] + [index2bert.get(token_id, 100) for token_id in batch] for batch in bert_inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "a27656f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1037,   103,  1997,  2273,  1998,  2308, 10955,  1998, 11278,  2012,\n",
      "         2712,  5370,     0,     0])\n",
      "tensor([ 1037,  2177,   103,  2273,  1998,  2308, 10955,  1998, 11278,  2012,\n",
      "         2712,  5370,     0,     0])\n",
      "tensor([ 1037,  2177,  1997,   103,  1998,  2308, 10955,  1998, 11278,  2012,\n",
      "         2712,  5370,     0,     0])\n"
     ]
    }
   ],
   "source": [
    "max_target_len = targets.size(0)\n",
    "\n",
    "bert_targets_np = np.frompyfunc(\n",
    "        lambda x: index2bert.get(x, 100) if x != 2 else index2bert.get(0, 0),\n",
    "        1, 1\n",
    "    )(targets.T.detach().cpu().numpy()).astype('int32')\n",
    "\n",
    "for t in range(3):\n",
    "    # this iteration will be the decoding steps\n",
    "    # and the input into the bert model\n",
    "    inputs_id = torch.tensor(bert_targets_np).long()\n",
    "    inputs_id[:, t+1][inputs_id[:, t+1] != 0] = 103\n",
    "    print(inputs_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb54e60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_tokenizer.convert_ids_to_tokens(inputs_id[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "d95d8885",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = torch.rand(32, 5044)\n",
    "# decoder_output = decoder_output.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "77d45f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 50)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk_indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "bc2182eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 30522])"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][:, t+1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "9f7e34b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.3000, 0.2000, 0.1000],\n",
      "        [0.3000, 0.2000, 0.1000]])\n",
      "tensor([[0.1000, 0.3000],\n",
      "        [0.2000, 0.2000],\n",
      "        [0.3000, 0.1000]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.1000, 0.1400],\n",
       "        [0.1000, 0.1400]])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q = torch.tensor([[0.3, 0.2, 0.1], \n",
    "                  [0.3, 0.2, 0.1]])\n",
    "print(q)\n",
    "p = torch.tensor([[0.1, 0.2, 0.3], \n",
    "                  [0.3, 0.2, 0.1]])\n",
    "print(p.T)\n",
    "torch.matmul(q, p.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "71d2bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature = 3\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(inputs_id)[0]\n",
    "    outputs = F.softmax((outputs / temperature), dim=-1)\n",
    "\n",
    "topk_bert_indices = torch.argsort(outputs, dim=-1, descending=True)[:, t+1, :50] # ambil predicted word pada timestep t\n",
    "# proba_q = torch.gather(outputs[:, t+1], 1, topk_bert_indices)\n",
    "\n",
    "topk_indices = torch.tensor(np.frompyfunc(\n",
    "        lambda x: bert2index.get(x, 3),1, 1)(topk_bert_indices.detach().cpu().numpy()).astype('int32')).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "5b953040",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import Utils\n",
    "\n",
    "utils = Utils()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "747a8bbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "KLLoss() got an unexpected keyword argument 't'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8804/1789574183.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mkl_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mKLLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdecoder_output\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopk_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopk_bert_indices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkl_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: KLLoss() got an unexpected keyword argument 't'"
     ]
    }
   ],
   "source": [
    "kl_loss = utils.KLLoss(decoder_output, outputs, topk_indices, topk_bert_indices, t=3)\n",
    "print(kl_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "225924e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLLoss(decoder_output, bert_output, topk_indices, topk_bert_indices):\n",
    "    log_loss_p = torch.log(torch.gather(decoder_output, 1, topk_indices))\n",
    "    proba_q = torch.gather(outputs[:, t+1], 1, topk_bert_indices)\n",
    "    kl_loss = (-(proba_q * log_loss_p).sum(dim=-1)).mean()\n",
    "    return kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "0cd192ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0180)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kl_loss = KLLoss(decoder_output, outputs, topk_indices, topk_bert_indices)\n",
    "kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a9c0e824",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a n d', 't e a', 't o', 'o r', 't r e y', 'w i t h', 'a', 'a c', 't a m', 'a l l', 's e v e r a l', '...', 'n o', 't h e n', 'a t', 'a l', '[ S E P ]', 's h e', 'n o r', '[ C L S ]']\n"
     ]
    }
   ],
   "source": [
    "predictions = outputs[0][0, 7].topk(k=20).indices.tolist()\n",
    "\n",
    "predicted_tokens = [my_tokenizer.decode(token_id) for token_id in predictions]\n",
    "print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "77b5e2c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted token: bingham\n"
     ]
    }
   ],
   "source": [
    "input_sequence = \"a woman with [MASK] teaching how to trim flowers\"\n",
    "\n",
    "# Tokenize the input sequence and get the index of the masked token\n",
    "input_tokens = my_tokenizer.tokenize(input_sequence)\n",
    "mask_index = input_tokens.index(\"[MASK]\")\n",
    "input_ids = my_tokenizer.convert_tokens_to_ids(input_tokens)\n",
    "\n",
    "# Replace the masked token with its corresponding ID\n",
    "input_ids[mask_index] = tokenizer.mask_token_id\n",
    "\n",
    "# Convert input IDs to PyTorch tensor and add a batch dimension\n",
    "input_tensor = torch.tensor([input_ids])\n",
    "\n",
    "# Predict the masked word\n",
    "with torch.no_grad():\n",
    "    output = model(input_tensor)\n",
    "\n",
    "predicted_token_id = torch.argmax(output[0][0][mask_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id)\n",
    "\n",
    "print(f\"Predicted token: {predicted_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "3e488fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b i n g h a m', 'r e i d', 'p r o', 'n a', 'i o n a', 'a d d i t i o n', 'p c', 'p e r h a p s', 'b r u c e', 'c l i n t o n', 'e f f e c t i v e l y', 'w o r k', 'f u r t h e r', 'a l s o', 'c o n g r e s s', 'g r o w t h', 'f r a s e r', 'b n', 'b y r n e', 'i m p a c t', 'f r', 'e a r l y', 'c o n t i n u e d', 'i n p u t', 'g r o u p', 'e d', 'a', 'h o l m e s', 's h o c k', 'h i m s e l f', 'c r e d i t', 'b o o k', 'a n d', 'c o r n e l l', 'd a v i d', 'l a t e r', 'f i r s t', 'e x p e r i e n c e', 'c o a', 'r e s e a r c h', 'e v i d e n c e', 'f i l i n g', 'n o', 'm a x', 'g r o w i n g', 's e r v i c e', 't o', 'c o n t i n u i n g', 't h i s', 'c b c']\n"
     ]
    }
   ],
   "source": [
    "predictions = output[0][0, mask_index].topk(k=50).indices.tolist()\n",
    "\n",
    "predicted_tokens = [my_tokenizer.decode(token_id) for token_id in predictions]\n",
    "print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f93d1a",
   "metadata": {},
   "source": [
    "## Suggestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "8c29bffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  103,  1037,  2177,  1997,  2273,  1998,  2308, 10955,  1998, 11278,\n",
      "         2012,  2712,  5370,   102,     0])\n",
      "tensor([  103,   103,  2177,  1997,  2273,  1998,  2308, 10955,  1998, 11278,\n",
      "         2012,  2712,  5370,   102,     0])\n",
      "tensor([  103,   103,   103,  1997,  2273,  1998,  2308, 10955,  1998, 11278,\n",
      "         2012,  2712,  5370,   102,     0])\n"
     ]
    }
   ],
   "source": [
    "max_target_len = targets.size(0)\n",
    "\n",
    "input_ids = torch.tensor([[101] + [index2bert.get(token_id, 100) for token_id in batch] for batch in bert_inputs])\n",
    "\n",
    "for t in range(max_target_len):\n",
    "    # this iteration will be the decoding steps\n",
    "    # and the input into the bert model\n",
    "    if t < 3:\n",
    "        input_ids[:, t].masked_fill_(input_ids[:, t] != 0, 103)\n",
    "        print(input_ids[0])\n",
    "    else:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "cc5b5186",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  101,  1037,  2177,  1997,  2273,  1998,  2308, 10955,  1998,\n",
       "       11278,  2012,  2712,  5370,   102,     0], dtype=int64)"
      ]
     },
     "execution_count": 478,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_targets_np[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314aba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "[my_tokenizer.convert_ids_to_tokens(bert_token) for bert_token in bert_inputs[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c7569cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I want to eat [MASK] for lunch today.\"\n",
    "\n",
    "tokenized_text = my_tokenizer.tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9abf30d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_index = len(tokenized_text) - 1\n",
    "tokenized_text[masked_index] = '[MASK]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "b1bda547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[PAD]', '[CLS]', 'backlash', 'mace', 'effectively', '##fare', 'excess', 'thanksgiving', 'class', 'multiple', 'alcohol', 'dish', 'chef', 'conflict', 'vlad', 'food', 'jared', 'chefs', 'violence', 'host']\n"
     ]
    }
   ],
   "source": [
    "# Convert tokens to IDs\n",
    "indexed_tokens = my_tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor)\n",
    "    predictions = outputs[0][0][4].topk(k=20).indices.tolist()\n",
    "    predicted_tokens = my_tokenizer.convert_ids_to_tokens(predictions)\n",
    "    print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038cf256",
   "metadata": {},
   "source": [
    "## Inference BertForMaskedLM Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "3fbd22bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "889bd610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tokenizer.add_tokens('[MASK]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "1201de36",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"a woman with green [MASK]\"\n",
    "\n",
    "# Tokenize input sentence\n",
    "tokens = my_tokenizer.tokenize(sentence)\n",
    "mask_idx = tokens.index('[MASK]')\n",
    "input_ids = torch.tensor([tokenizer.encode(sentence, add_special_tokens=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "6a4b0ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 101, 1037, 2450, 2007, 2665,  103,  102]])"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35d979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    predictions = outputs[0][0, mask_idx].topk(k=50).indices.tolist()\n",
    "\n",
    "predicted_tokens = [tokenizer.decode(token_id) for token_id in predictions]\n",
    "print(predicted_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "549d1d96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 7, 30522])"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3efc14",
   "metadata": {},
   "source": [
    "Jadi index [0] pertama itu untuk mengambil tensor logits. Lalu [0] kedua adalah untuk mengambil batch, karena batchnya hanya satu untuk saat ini. Lalu mask_idx adalah dimensi indeks token yang diambil."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "b27b3df4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-2.2777, -7.6340, -7.9684,  ..., -7.5812, -5.0998, -8.6828])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][0, mask_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71cc8243",
   "metadata": {},
   "source": [
    "## KL Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "9969d848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 30522])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7a872",
   "metadata": {},
   "source": [
    "## Miscelaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d696b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "[index2word.get(idx) for idx in targets[0].cpu().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b841ea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = model(bert_t.view(-1, 1))\n",
    "    \n",
    "bert_indices = torch.argsort(logits[0].squeeze(1), dim=1)[:, :50].long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "df6f62f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[17007, 27004, 28878,  ..., 26331, 27690, 17181],\n",
       "        [24229, 22968, 28397,  ..., 17766, 11656, 29504],\n",
       "        [23695, 28666, 18926,  ..., 25393, 11300, 27388],\n",
       "        ...,\n",
       "        [25495, 13900, 17661,  ..., 25027, 15994, 21322],\n",
       "        [22925, 29553, 23630,  ..., 19859, 19948, 21273],\n",
       "        [27920, 18926, 14918,  ..., 12335, 27245, 17007]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "36471902",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "num_words = 50\n",
    "\n",
    "default_value = torch.full((batch_size, num_words), 3, dtype=torch.long)\n",
    "bert_indices = bert_indices.long()\n",
    "soft_predict_index = torch.gather(default_value, 1, bert2index.get(bert_indices, default_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "70e5846b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soft_predict_index.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b68607b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_output = torch.rand(32, 5044)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b3cfd76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 50])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_indices.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ff750d",
   "metadata": {},
   "source": [
    "## Miscellaneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8976d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(bert_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "a33044d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = model(bert_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "352c8a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.argsort(result[0].squeeze(1), dim=-1, descending=True)[:, :50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "fe11f414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0,    0,  102,    0,  102,    0,    0,    0,    0,    0, 1998,    0,\n",
       "           0,  102,    0,    0,    0,    0,    0, 4183,  102,  101,  101,    0,\n",
       "           0,    0,    0,    0,    0,    0,  102,    0])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_indices = torch.argmax(result[0].squeeze(1), dim=-1)\n",
    "max_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ce60c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(indices[19])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
